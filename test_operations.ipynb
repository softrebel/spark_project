{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark import SparkConf, SparkContext\n",
    "from pyspark.sql.types import StructType,IntegerType,FloatType,BooleanType,StringType\n",
    "from pyspark.sql.functions import rand\n",
    "conf = SparkConf().setMaster(\"local[*]\").setAppName(\"My App\")\n",
    "sc = SparkContext.getOrCreate(conf = conf)\n",
    "sc._conf.set('spark.executor.memory','15g')\\\n",
    "    .set('spark.driver.memory','15g')\\\n",
    "        .set('spark.driver.maxResultsSize','0')\n",
    "spark=SparkSession.builder\\\n",
    "    .appName('myApp')\\\n",
    "        .config(\"spark.driver.memory\", \"15g\")\\\n",
    "            .getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data(files,schema):\n",
    "    df=spark.read.csv(files,header=True\n",
    "                  ,schema=schema)\n",
    "    return df\n",
    "\n",
    "def load_record_linkage_data():\n",
    "    schema = StructType() \\\n",
    "      .add(\"id_1\",IntegerType(),True) \\\n",
    "      .add(\"id_2\",IntegerType(),True) \\\n",
    "      .add(\"cmp_fname_c1\",FloatType(),True) \\\n",
    "      .add(\"cmp_fname_c2\",FloatType(),True) \\\n",
    "      .add(\"cmp_lname_c1\",FloatType(),True) \\\n",
    "      .add(\"cmp_lname_c2\",FloatType(),True) \\\n",
    "      .add(\"cmp_sex\",IntegerType(),True) \\\n",
    "      .add(\"cmp_bd\",IntegerType(),True) \\\n",
    "      .add(\"cmp_bm\",IntegerType(),True) \\\n",
    "      .add(\"cmp_by\",IntegerType(),True) \\\n",
    "      .add(\"cmp_plz\",IntegerType(),True) \\\n",
    "      .add(\"is_match\",BooleanType(),False)\n",
    "    files=[f'./data/block_{id}.csv' for id in range(1,11)]\n",
    "    return load_data(files,schema=schema)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "df=load_record_linkage_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+-------+\n",
      "|cmp_plz|  count|\n",
      "+-------+-------+\n",
      "|   null|  12843|\n",
      "|      1|  31714|\n",
      "|      0|5704575|\n",
      "+-------+-------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.groupBy('cmp_plz').count().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5749132"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- id_1: integer (nullable = true)\n",
      " |-- id_2: integer (nullable = true)\n",
      " |-- cmp_fname_c1: float (nullable = true)\n",
      " |-- cmp_fname_c2: float (nullable = true)\n",
      " |-- cmp_lname_c1: float (nullable = true)\n",
      " |-- cmp_lname_c2: float (nullable = true)\n",
      " |-- cmp_sex: integer (nullable = true)\n",
      " |-- cmp_bd: integer (nullable = true)\n",
      " |-- cmp_bm: integer (nullable = true)\n",
      " |-- cmp_by: integer (nullable = true)\n",
      " |-- cmp_plz: integer (nullable = true)\n",
      " |-- is_match: boolean (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.printSchema()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Missing Values"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Drop Missing Values\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "miss_df=df.drop('id_1','id_2')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "اگر همه داده های گم شده را حذف کنیم، کلا 20 رکورد باقی می‌ ماند !\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "20"
      ]
     },
     "execution_count": 117,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "miss_df.replace('?',None).na.drop().count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "miss_df=miss_df.replace('?',None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(cmp_fname_c1=0.8333333134651184, cmp_fname_c2=None, cmp_lname_c1=1.0, cmp_lname_c2=None, cmp_sex=1, cmp_bd=1, cmp_bm=1, cmp_by=1, cmp_plz=0, is_match=True),\n",
       " Row(cmp_fname_c1=1.0, cmp_fname_c2=None, cmp_lname_c1=1.0, cmp_lname_c2=None, cmp_sex=1, cmp_bd=1, cmp_bm=1, cmp_by=1, cmp_plz=1, is_match=True),\n",
       " Row(cmp_fname_c1=1.0, cmp_fname_c2=None, cmp_lname_c1=1.0, cmp_lname_c2=None, cmp_sex=1, cmp_bd=1, cmp_bm=1, cmp_by=1, cmp_plz=1, is_match=True)]"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "miss_df.head(3)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "هیچ رکوردی که همه یا حداقل 2 تا از متغیرهای آن گم شده باشد، وجود ندارد"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5749132"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "miss_df.na.drop(how='all').count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5749132"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "miss_df.na.drop(how='any',thresh=2).count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------+------------+------------+------------+-------+------+------+------+-------+--------+\n",
      "|cmp_fname_c1|cmp_fname_c2|cmp_lname_c1|cmp_lname_c2|cmp_sex|cmp_bd|cmp_bm|cmp_by|cmp_plz|is_match|\n",
      "+------------+------------+------------+------------+-------+------+------+------+-------+--------+\n",
      "+------------+------------+------------+------------+-------+------+------+------+-------+--------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "miss_df.filter(miss_df.cmp_lname_c1==None).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------+------+\n",
      "|cmp_fname_c1| count|\n",
      "+------------+------+\n",
      "|  0.27272728|   454|\n",
      "|   0.8181818|     3|\n",
      "|  0.16666667|152732|\n",
      "|        0.25|137039|\n",
      "|       0.875| 71211|\n",
      "|   0.5714286|  7414|\n",
      "|  0.47058824|    11|\n",
      "|        null|  1007|\n",
      "|        0.75| 46521|\n",
      "|         0.1| 10357|\n",
      "|  0.11111111|123127|\n",
      "|       0.125|155172|\n",
      "|  0.36363637|   293|\n",
      "|   0.7777778|  3083|\n",
      "|         0.6| 19725|\n",
      "|         0.9|  7780|\n",
      "|         0.5| 44615|\n",
      "|  0.42857143| 34463|\n",
      "|   0.2857143| 78429|\n",
      "|  0.33333334| 94936|\n",
      "+------------+------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "miss_df.groupBy('cmp_fname_c1').count().show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fill the Missing Values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.feature import Imputer\n",
    "from pyspark.sql.functions import when, lit\n",
    "# for float variables\n",
    "\n",
    "\n",
    "\n",
    "def convert_label_binary(input_df):\n",
    "    temp = input_df.withColumn('label',\n",
    "                             when(input_df['is_match']==True,\n",
    "                                  lit(1)).otherwise(0)\n",
    "                                  ) \n",
    "    return temp\n",
    "\n",
    "def fill_missing_values(input_df):\n",
    "    miss_df=input_df.drop('id_1','id_2')\n",
    "    miss_df=miss_df.replace('?',None)\n",
    "    float_cols=[\n",
    "    'cmp_fname_c1', \n",
    "    'cmp_fname_c2', \n",
    "    'cmp_lname_c1', \n",
    "    'cmp_lname_c2', \n",
    "    ]\n",
    "    float_imputer = Imputer(\n",
    "        inputCols=float_cols,\n",
    "        outputCols=[f\"{col}_imputed\" for col in float_cols]\n",
    "    ).setStrategy('mean')\n",
    "\n",
    "    # for binary variables\n",
    "    binary_cols=[\n",
    "        'cmp_sex', \n",
    "        'cmp_bd', \n",
    "        'cmp_bm', \n",
    "        'cmp_by',\n",
    "        'cmp_plz',\n",
    "    ]\n",
    "    binary_imputer = Imputer(\n",
    "        inputCols=binary_cols,\n",
    "        outputCols=[f\"{col}_imputed\" for col in binary_cols]\n",
    "    ).setStrategy('mode')\n",
    "    imputed_df=float_imputer.fit(miss_df).transform(miss_df)\n",
    "    output_df=binary_imputer.fit(imputed_df).transform(imputed_df)\n",
    "    output_df=output_df.select([x for x in output_df.columns if '_imputed' in x or x=='is_match'])\n",
    "    return output_df\n",
    "\n",
    "\n",
    "def preprocessing_df(input_df):\n",
    "    return convert_label_binary(fill_missing_values(input_df))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "prep_df=preprocessing_df(df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5749132"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prep_df.na.drop().count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- is_match: boolean (nullable = true)\n",
      " |-- cmp_fname_c1_imputed: float (nullable = true)\n",
      " |-- cmp_fname_c2_imputed: float (nullable = true)\n",
      " |-- cmp_lname_c1_imputed: float (nullable = true)\n",
      " |-- cmp_lname_c2_imputed: float (nullable = true)\n",
      " |-- cmp_sex_imputed: integer (nullable = true)\n",
      " |-- cmp_bd_imputed: integer (nullable = true)\n",
      " |-- cmp_bm_imputed: integer (nullable = true)\n",
      " |-- cmp_by_imputed: integer (nullable = true)\n",
      " |-- cmp_plz_imputed: integer (nullable = true)\n",
      " |-- label: integer (nullable = false)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "prep_df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import col\n",
    "prep_dff=prep_df.drop(col('is_match'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- cmp_fname_c1_imputed: float (nullable = true)\n",
      " |-- cmp_fname_c2_imputed: float (nullable = true)\n",
      " |-- cmp_lname_c1_imputed: float (nullable = true)\n",
      " |-- cmp_lname_c2_imputed: float (nullable = true)\n",
      " |-- cmp_sex_imputed: integer (nullable = true)\n",
      " |-- cmp_bd_imputed: integer (nullable = true)\n",
      " |-- cmp_bm_imputed: integer (nullable = true)\n",
      " |-- cmp_by_imputed: integer (nullable = true)\n",
      " |-- cmp_plz_imputed: integer (nullable = true)\n",
      " |-- label: integer (nullable = false)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "prep_dff.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+-------+\n",
      "|cmp_fname_c1_imputed|  count|\n",
      "+--------------------+-------+\n",
      "|          0.27272728|    454|\n",
      "|           0.8181818|      3|\n",
      "|          0.16666667| 152732|\n",
      "|                0.25| 137039|\n",
      "|               0.875|  71211|\n",
      "|           0.5714286|   7414|\n",
      "|          0.47058824|     11|\n",
      "|                0.75|  46521|\n",
      "|                 0.1|  10357|\n",
      "|          0.11111111| 123127|\n",
      "|               0.125| 155172|\n",
      "|          0.36363637|    293|\n",
      "|           0.7777778|   3083|\n",
      "|                 0.6|  19725|\n",
      "|                 0.9|   7780|\n",
      "|                 0.5|  44615|\n",
      "|          0.42857143|  34463|\n",
      "|           0.2857143|  78429|\n",
      "|          0.33333334|  94936|\n",
      "|                 1.0|3508203|\n",
      "+--------------------+-------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "prep_df.groupBy('cmp_fname_c1_imputed').count().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+--------------------+--------------------+--------------------+--------------------+---------------+--------------+--------------+--------------+---------------+-----+\n",
      "|is_match|cmp_fname_c1_imputed|cmp_fname_c2_imputed|cmp_lname_c1_imputed|cmp_lname_c2_imputed|cmp_sex_imputed|cmp_bd_imputed|cmp_bm_imputed|cmp_by_imputed|cmp_plz_imputed|label|\n",
      "+--------+--------------------+--------------------+--------------------+--------------------+---------------+--------------+--------------+--------------+---------------+-----+\n",
      "|true    |0.8333333           |0.9000177           |1.0                 |0.31841284          |1              |1             |1             |1             |0              |1    |\n",
      "+--------+--------------------+--------------------+--------------------+--------------------+---------------+--------------+--------------+--------------+---------------+-----+\n",
      "only showing top 1 row\n",
      "\n"
     ]
    }
   ],
   "source": [
    "prep_df.show(1,truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+--------------------+--------------------+--------------------+--------------------+---------------+--------------+--------------+--------------+---------------+-----+\n",
      "|is_match|cmp_fname_c1_imputed|cmp_fname_c2_imputed|cmp_lname_c1_imputed|cmp_lname_c2_imputed|cmp_sex_imputed|cmp_bd_imputed|cmp_bm_imputed|cmp_by_imputed|cmp_plz_imputed|label|\n",
      "+--------+--------------------+--------------------+--------------------+--------------------+---------------+--------------+--------------+--------------+---------------+-----+\n",
      "|true    |0.8333333           |0.9000177           |1.0                 |0.31841284          |1              |1             |1             |1             |0              |1    |\n",
      "|true    |1.0                 |0.9000177           |1.0                 |0.31841284          |1              |1             |1             |1             |1              |1    |\n",
      "|true    |1.0                 |0.9000177           |1.0                 |0.31841284          |1              |1             |1             |1             |1              |1    |\n",
      "+--------+--------------------+--------------------+--------------------+--------------------+---------------+--------------+--------------+--------------+---------------+-----+\n",
      "only showing top 3 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "prep_df[prep_df['label']>0].show(3,truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.feature import VectorAssembler\n",
    "def feature_engineering(input_df,feature_list,label_name):\n",
    "    assembler = VectorAssembler(inputCols=feature_list,\n",
    "                             outputCol='features')\n",
    "    assembled_df = assembler.transform(input_df)\n",
    "    output_df=assembled_df.select('features', label_name)\n",
    "    return output_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['cmp_lname_c2_imputed',\n",
       " 'cmp_bd_imputed',\n",
       " 'cmp_sex_imputed',\n",
       " 'cmp_plz_imputed',\n",
       " 'cmp_fname_c1_imputed',\n",
       " 'cmp_lname_c1_imputed',\n",
       " 'cmp_fname_c2_imputed',\n",
       " 'cmp_by_imputed',\n",
       " 'cmp_bm_imputed']"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "features=list(set(prep_dff.columns) - set(['label','is_match']))\n",
    "features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+--------------------+--------------+--------------------+--------------------+---------------+---------------+--------------+--------------+\n",
      "|cmp_lname_c2_imputed|cmp_lname_c1_imputed|cmp_by_imputed|cmp_fname_c2_imputed|cmp_fname_c1_imputed|cmp_sex_imputed|cmp_plz_imputed|cmp_bm_imputed|cmp_bd_imputed|\n",
      "+--------------------+--------------------+--------------+--------------------+--------------------+---------------+---------------+--------------+--------------+\n",
      "|0.31841284          |1.0                 |1             |0.9000177           |0.8333333           |1              |0              |1             |1             |\n",
      "|0.31841284          |1.0                 |1             |0.9000177           |1.0                 |1              |1              |1             |1             |\n",
      "|0.31841284          |1.0                 |1             |0.9000177           |1.0                 |1              |1              |1             |1             |\n",
      "|0.31841284          |1.0                 |1             |0.9000177           |1.0                 |1              |1              |1             |1             |\n",
      "|1.0                 |1.0                 |1             |0.9000177           |1.0                 |1              |1              |1             |1             |\n",
      "|0.31841284          |1.0                 |1             |0.9000177           |1.0                 |1              |1              |1             |1             |\n",
      "|0.31841284          |1.0                 |1             |0.9000177           |1.0                 |1              |1              |1             |1             |\n",
      "|0.31841284          |1.0                 |1             |0.9000177           |1.0                 |1              |0              |1             |1             |\n",
      "|0.31841284          |1.0                 |1             |0.9000177           |1.0                 |1              |1              |1             |1             |\n",
      "|0.31841284          |1.0                 |1             |0.9000177           |1.0                 |1              |1              |1             |1             |\n",
      "+--------------------+--------------------+--------------+--------------------+--------------------+---------------+---------------+--------------+--------------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import element_at\n",
    "prep_df.select(features).show(10,truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------------------------------------------------------------------------------+-----+\n",
      "|features                                                                          |label|\n",
      "+----------------------------------------------------------------------------------+-----+\n",
      "|[0.3184128403663635,1.0,1.0,0.0,0.8333333134651184,1.0,0.9000176787376404,1.0,1.0]|1    |\n",
      "|[0.3184128403663635,1.0,1.0,1.0,1.0,1.0,0.9000176787376404,1.0,1.0]               |1    |\n",
      "|[0.3184128403663635,1.0,1.0,1.0,1.0,1.0,0.9000176787376404,1.0,1.0]               |1    |\n",
      "|[0.3184128403663635,1.0,1.0,1.0,1.0,1.0,0.9000176787376404,1.0,1.0]               |1    |\n",
      "|[1.0,1.0,1.0,1.0,1.0,1.0,0.9000176787376404,1.0,1.0]                              |1    |\n",
      "+----------------------------------------------------------------------------------+-----+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "assembled_df = feature_engineering(prep_dff,features,'label')\n",
    "assembled_df.show(5, truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------------------------------------------+-----+\n",
      "|features                                      |label|\n",
      "+----------------------------------------------+-----+\n",
      "|(9,[0,1,2,4],[0.3184128403663635,1.0,1.0,1.0])|0    |\n",
      "+----------------------------------------------+-----+\n",
      "only showing top 1 row\n",
      "\n"
     ]
    }
   ],
   "source": [
    "r=assembled_df.randomSplit([0.8,0.2],22)\n",
    "r[0].show(1,truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------------------------------------------------+-----+\n",
      "|features                                                     |label|\n",
      "+-------------------------------------------------------------+-----+\n",
      "|(9,[0,1,2,4],[0.3184128403663635,1.0,1.0,1.0])               |0    |\n",
      "|(9,[0,1,2,5],[0.3184128403663635,1.0,1.0,0.4285714328289032])|0    |\n",
      "|(9,[0,1,2,5],[0.3184128403663635,1.0,1.0,1.0])               |0    |\n",
      "|(9,[0,1,2,6],[0.3184128403663635,1.0,1.0,0.9000176787376404])|0    |\n",
      "|(9,[0,1,2,6],[0.3184128403663635,1.0,1.0,0.9000176787376404])|0    |\n",
      "|(9,[0,1,2,6],[0.3184128403663635,1.0,1.0,0.9000176787376404])|0    |\n",
      "|(9,[0,1,2,6],[0.3184128403663635,1.0,1.0,0.9000176787376404])|0    |\n",
      "|(9,[0,1,2,6],[0.3184128403663635,1.0,1.0,0.9000176787376404])|0    |\n",
      "|(9,[0,1,2,6],[0.3184128403663635,1.0,1.0,0.9000176787376404])|0    |\n",
      "|(9,[0,1,2,6],[0.3184128403663635,1.0,1.0,0.9000176787376404])|0    |\n",
      "+-------------------------------------------------------------+-----+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "r[0].show(10,truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "df3 = assembled_df.withColumn('rand', rand(seed=42)).orderBy('rand')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------------------------------------------------------------------------------+-----+---------------------+\n",
      "|features                                                                          |label|rand                 |\n",
      "+----------------------------------------------------------------------------------+-----+---------------------+\n",
      "|[0.3184128403663635,0.0,1.0,0.0,1.0,0.3636363744735718,0.9000176787376404,0.0,1.0]|0    |1.0589978002295553E-6|\n",
      "|[0.3184128403663635,0.0,1.0,0.0,1.0,0.1428571492433548,0.9000176787376404,1.0,0.0]|0    |1.1244146868039095E-6|\n",
      "|(9,[0,2,5,6],[0.3184128403663635,1.0,0.8571428656578064,0.9000176787376404])      |0    |1.8169130715506299E-6|\n",
      "+----------------------------------------------------------------------------------+-----+---------------------+\n",
      "only showing top 3 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df3.show(3,truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------------------------------------------------------------------------------+-----+---------------------+\n",
      "|features                                                                          |label|rand                 |\n",
      "+----------------------------------------------------------------------------------+-----+---------------------+\n",
      "|[0.3184128403663635,0.3636363744735718,0.0,0.9000176787376404,1.0,1.0,0.0,1.0,0.0]|0    |1.0589978002295553E-6|\n",
      "|[0.3184128403663635,0.1428571492433548,1.0,0.9000176787376404,1.0,1.0,0.0,0.0,0.0]|0    |1.1244146868039095E-6|\n",
      "|(9,[0,1,3,5],[0.3184128403663635,0.8571428656578064,0.9000176787376404,1.0])      |0    |1.8169130715506299E-6|\n",
      "+----------------------------------------------------------------------------------+-----+---------------------+\n",
      "only showing top 3 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_train=df3.filter(df3.rand < 0.3)\n",
    "df_test=df3.filter(df3.rand >= 0.3)\n",
    "df_train.show(3,truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------------------------------------------------------------------------------------+-----+-------------------+\n",
      "|features                                                                                         |label|rand               |\n",
      "+-------------------------------------------------------------------------------------------------+-----+-------------------+\n",
      "|[0.3184128403663635,0.8888888955116272,0.0,0.9000176787376404,0.2857142984867096,1.0,0.0,1.0,0.0]|0    |0.30000020961157103|\n",
      "|(9,[0,1,4,5],[0.3184128403663635,0.20000000298023224,0.25,1.0])                                  |0    |0.3000004588256735 |\n",
      "|[0.3184128403663635,0.5555555820465088,0.0,0.9000176787376404,1.0,1.0,0.0,0.0,0.0]               |0    |0.3000007368526143 |\n",
      "+-------------------------------------------------------------------------------------------------+-----+-------------------+\n",
      "only showing top 3 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_test.show(3,truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_train_split(input_df,train_size=0.7):\n",
    "    train, test = assembled_df.randomSplit([train_size,1 - train_size], seed=42)\n",
    "    return train,test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train, test = test_train_split(assembled_df,0.7)\n",
    "train.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prep_df.groupBy('label').count().show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ML Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.mllib.evaluation import BinaryClassificationMetrics\n",
    "from pyspark.ml.evaluation import BinaryClassificationEvaluator,MulticlassClassificationEvaluator\n",
    "from pyspark.ml.classification import DecisionTreeClassifier,LogisticRegression\n",
    "from pyspark.ml import Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import roc_curve, auc\n",
    "import numpy as np\n",
    "def evaluate_from_scratch(pred,model_name='Logistic Regression'):\n",
    "    pred.groupBy('label', 'prediction').count().show()\n",
    "\n",
    "    # Calculate the elements of the confusion matrix\n",
    "    TN = pred.filter('prediction = 0 AND label = prediction').count()\n",
    "    TP = pred.filter('prediction = 1 AND label = prediction').count()\n",
    "    FN = pred.filter('prediction = 0 AND label = 1').count()\n",
    "    FP = pred.filter('prediction = 1 AND label = 0').count()\n",
    "\n",
    "    # Accuracy measures the proportion of correct predictions\n",
    "    accuracy = (TN + TP) / (TN + TP + FN + FP)\n",
    "    recall = (TP) / (TP+FN)\n",
    "    precision= (TP) / (TP+FP)\n",
    "    f1=2*(precision*recall)/(precision+recall)\n",
    "    print(f'EVALUATION SUMMARY for {model_name}:')\n",
    "    print(f\" accuracy:{accuracy}\")\n",
    "    print(f\" precision:{precision}\")\n",
    "    print(f\" recall:{recall}\")\n",
    "    print(f\" f1-score:{f1}\")\n",
    "\n",
    "def evaluate_from_spark(predictions,model_name='Logistic Regression'):\n",
    "    eval = BinaryClassificationEvaluator(rawPredictionCol=\"probability\", labelCol=\"label\")\n",
    "    eval2= MulticlassClassificationEvaluator(predictionCol=\"prediction\", labelCol=\"label\")\n",
    "    AUC  = eval.evaluate(predictions)\n",
    "    ACC  = eval2.evaluate(predictions, {eval2.metricName:\"accuracy\"})\n",
    "    PREC  = eval2.evaluate(predictions, {eval2.metricName:\"weightedPrecision\"})\n",
    "    REC  = eval2.evaluate(predictions, {eval2.metricName:\"weightedRecall\"})\n",
    "    F1  = eval2.evaluate(predictions, {eval2.metricName:\"f1\"})\n",
    "    WeightedFMeasure=eval2.evaluate(predictions, {eval2.metricName:\"weightedFMeasure\"})\n",
    "    print(f\"{model_name} Performance Measure\")\n",
    "    print(\" Accuracy = %0.8f\" % ACC)\n",
    "    print(\" Weighted Precision = %0.8f\" % PREC)\n",
    "    print(\" Weighted Recall = %0.8f\" % REC)\n",
    "    print(\" F1 = %0.8f\" % F1)\n",
    "    print(\" Weighted F Measure = %0.8f\" % WeightedFMeasure)\n",
    "\n",
    "    print(\" AUC = %.8f\" % AUC)\n",
    "    print(\" ROC curve:\")\n",
    "    PredAndLabels           = predictions.select(\"probability\", \"label\")\n",
    "    PredAndLabels_collect   = PredAndLabels.collect()\n",
    "    PredAndLabels_list      = [(float(i[0][0]), 1.0-float(i[1])) for i in PredAndLabels_collect]\n",
    "    PredAndLabels           = sc.parallelize(PredAndLabels_list)\n",
    "    fpr = dict()                                                        # FPR: False Positive Rate\n",
    "    tpr = dict()                                                        # TPR: True Positive Rate\n",
    "    roc_auc = dict()\n",
    "    \n",
    "    y_test = [i[1] for i in PredAndLabels_list]\n",
    "    y_score = [i[0] for i in PredAndLabels_list]\n",
    "    \n",
    "    fpr, tpr, _ = roc_curve(y_test, y_score)\n",
    "    roc_auc = auc(fpr, tpr)\n",
    "    \n",
    "    plt.figure(figsize=(5,4))\n",
    "    plt.plot(fpr, tpr, label='ROC curve (area = %0.8f)' % roc_auc)\n",
    "    plt.plot([0, 1], [0, 1], 'r--')\n",
    "    # plt.xlim([0.0, 1.0])\n",
    "    # plt.ylim([0.0, 1.05])\n",
    "    plt.yticks(np.arange(0,1.03,0.1))\n",
    "    plt.xlabel('False Positive Rate')\n",
    "    plt.ylabel('True Positive Rate')\n",
    "    plt.title(f'ROC Curve - {model_name}')\n",
    "    plt.legend(loc=\"lower right\")\n",
    "    plt.show()\n",
    "\n",
    "    \n",
    "\n",
    "def evaluate(predictions,model_name=None):\n",
    "    print('Evaluate From Scratch:')\n",
    "    evaluate_from_scratch(predictions,model_name)\n",
    "    print('\\nEvaluate From Spark Library:')\n",
    "    evaluate_from_spark(predictions,model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# lr=LogisticRegression(featuresCol='features', labelCol='label')\n",
    "# lr_model = lr.fit(train)\n",
    "# lr_result = lr_model.transform(test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr=LogisticRegression(featuresCol='features', labelCol='label')\n",
    "pipeline = Pipeline(stages=[lr])\n",
    "model = pipeline.fit(train)\n",
    "lr_result = model.transform(test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr_result.select('label', 'prediction', 'probability').show(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluate(lr_result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Decision Tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tree = DecisionTreeClassifier()\n",
    "tree_pipeline = Pipeline(stages=[tree])\n",
    "tree_model = tree_pipeline.fit(train)\n",
    "tree_result = tree_model.transform(test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluate(tree_result,model_name='Decision Tree')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### RandomForest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.classification import RandomForestClassifier\n",
    "rf = RandomForestClassifier(labelCol=\"label\", featuresCol=\"features\")\n",
    "rf_pipeline = Pipeline(stages=[ rf])\n",
    "rf_model=rf_pipeline.fit(train)\n",
    "rf_result=rf_model.transform(test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluate(rf_result,model_name='Random Forest')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Not Refactored yet...\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cross Validation "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.tuning import CrossValidator, ParamGridBuilder, CrossValidatorModel\n",
    "from pyspark.ml.evaluation import BinaryClassificationEvaluator\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr_cs = LogisticRegression()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.classification import LogisticRegression\n",
    "lr_cs = LogisticRegression()\n",
    "grid = ParamGridBuilder().addGrid(lr_cs.regParam, [0.1, 0.01]) \\\n",
    "    .addGrid(lr_cs.fitIntercept, [False, True])\\\n",
    "    .addGrid(lr_cs.elasticNetParam, [0.0, 1.0])\\\n",
    "    .build()\n",
    "evaluator = BinaryClassificationEvaluator()\n",
    "cv = CrossValidator(estimator=lr_cs, estimatorParamMaps=grid, evaluator=evaluator)\n",
    "cvModel = cv.fit(train)\n",
    "lrprediction=cvModel.transform(test)\n",
    "\n",
    "\n",
    "\n",
    "print('Accuracy:', evaluator.evaluate(lrprediction))\n",
    "print('AUC:', BinaryClassificationMetrics(lrprediction['label','prediction'].rdd).areaUnderROC)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluate(lrprediction)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.classification import DecisionTreeClassifier\n",
    "\n",
    "dt = DecisionTreeClassifier()\n",
    "grid = ParamGridBuilder() \\\n",
    "        .addGrid(dt.maxDepth,  [2, 5, 10, 20, 30]) \\\n",
    "        .addGrid(dt.maxBins,  [10, 20, 40, 80, 100]) \\\n",
    "        .build()\n",
    "dtevaluator = BinaryClassificationEvaluator(rawPredictionCol=\"rawPrediction\")\n",
    "cv = CrossValidator(estimator=dt, \n",
    "                    estimatorParamMaps=grid, \n",
    "                    evaluator=dtevaluator,\n",
    "                    numFolds = 3)\n",
    "dtModel = cv.fit(train)\n",
    "dtpredictions = dtModel.transform(test)\n",
    "\n",
    "print('Accuracy:', dtevaluator.evaluate(dtpredictions))\n",
    "print('AUC:', BinaryClassificationMetrics(dtpredictions['label','prediction'].rdd).areaUnderROC)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluate(dtpredictions)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "6ed0a4610ee5d358854c63a9c9df0be609d14f26b47cd9f541e7269cbaa1b618"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
