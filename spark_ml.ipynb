{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark import SparkConf, SparkContext\n",
    "conf = SparkConf().setMaster(\"local[*]\").setAppName(\"My App\")\n",
    "sc = SparkContext.getOrCreate(conf = conf)\n",
    "spark=SparkSession.builder.appName('myApp').getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.types import StructType,FloatType,IntegerType,BooleanType\n",
    "\n",
    "schema = StructType() \\\n",
    "      .add(\"id_1\",IntegerType(),True) \\\n",
    "      .add(\"id_2\",IntegerType(),True) \\\n",
    "      .add(\"cmp_fname_c1\",FloatType(),True) \\\n",
    "      .add(\"cmp_fname_c2\",FloatType(),True) \\\n",
    "      .add(\"cmp_lname_c1\",FloatType(),True) \\\n",
    "      .add(\"cmp_lname_c2\",FloatType(),True) \\\n",
    "      .add(\"cmp_sex\",IntegerType(),True) \\\n",
    "      .add(\"cmp_bd\",IntegerType(),True) \\\n",
    "      .add(\"cmp_bm\",IntegerType(),True) \\\n",
    "      .add(\"cmp_by\",IntegerType(),True) \\\n",
    "      .add(\"cmp_plz\",IntegerType(),True) \\\n",
    "      .add(\"is_match\",BooleanType(),False)\n",
    "     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "files=[f'./data/block_{id}.csv' for id in range(1,11)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "df=spark.read.csv(files,header=True\n",
    "                  ,schema=schema)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+-------+\n",
      "|cmp_plz|  count|\n",
      "+-------+-------+\n",
      "|   null|  12843|\n",
      "|      1|  31714|\n",
      "|      0|5704575|\n",
      "+-------+-------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.groupBy('cmp_plz').count().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5749132"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- id_1: integer (nullable = true)\n",
      " |-- id_2: integer (nullable = true)\n",
      " |-- cmp_fname_c1: float (nullable = true)\n",
      " |-- cmp_fname_c2: float (nullable = true)\n",
      " |-- cmp_lname_c1: float (nullable = true)\n",
      " |-- cmp_lname_c2: float (nullable = true)\n",
      " |-- cmp_sex: integer (nullable = true)\n",
      " |-- cmp_bd: integer (nullable = true)\n",
      " |-- cmp_bm: integer (nullable = true)\n",
      " |-- cmp_by: integer (nullable = true)\n",
      " |-- cmp_plz: integer (nullable = true)\n",
      " |-- is_match: boolean (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.printSchema()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Missing Values"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Drop Missing Values\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "miss_df=df.drop('id_1','id_2')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "اگر همه داده های گم شده را حذف کنیم، کلا 20 رکورد باقی می‌ ماند !\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "20"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "miss_df.replace('?',None).na.drop().count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "miss_df=miss_df.replace('?',None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(cmp_fname_c1=0.8333333134651184, cmp_fname_c2=None, cmp_lname_c1=1.0, cmp_lname_c2=None, cmp_sex=1, cmp_bd=1, cmp_bm=1, cmp_by=1, cmp_plz=0, is_match=True),\n",
       " Row(cmp_fname_c1=1.0, cmp_fname_c2=None, cmp_lname_c1=1.0, cmp_lname_c2=None, cmp_sex=1, cmp_bd=1, cmp_bm=1, cmp_by=1, cmp_plz=1, is_match=True),\n",
       " Row(cmp_fname_c1=1.0, cmp_fname_c2=None, cmp_lname_c1=1.0, cmp_lname_c2=None, cmp_sex=1, cmp_bd=1, cmp_bm=1, cmp_by=1, cmp_plz=1, is_match=True)]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "miss_df.head(3)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "هیچ رکوردی که همه یا حداقل 2 تا از متغیرهای آن گم شده باشد، وجود ندارد"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5749132"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "miss_df.na.drop(how='all').count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5749132"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "miss_df.na.drop(how='any',thresh=2).count()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fill the Missing Values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.feature import Imputer\n",
    "# for float variables\n",
    "float_cols=[\n",
    "    'cmp_fname_c1', \n",
    "    'cmp_fname_c2', \n",
    "    'cmp_lname_c1', \n",
    "    'cmp_lname_c2', \n",
    "]\n",
    "float_imputer = Imputer(\n",
    "    inputCols=float_cols,\n",
    "    outputCols=[f\"{col}_imputed\" for col in float_cols]\n",
    ").setStrategy('mean')\n",
    "\n",
    "# for binary variables\n",
    "binary_cols=[\n",
    "    'cmp_sex', \n",
    "    'cmp_bd', \n",
    "    'cmp_bm', \n",
    "    'cmp_by',\n",
    "    'cmp_plz',\n",
    "]\n",
    "binary_imputer = Imputer(\n",
    "    inputCols=binary_cols,\n",
    "    outputCols=[f\"{col}_imputed\" for col in binary_cols]\n",
    ").setStrategy('mode')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "imputed_df=float_imputer.fit(miss_df).transform(miss_df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_imputed_df=binary_imputer.fit(imputed_df).transform(imputed_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------+------------+------------+------------+-------+------+------+------+-------+--------+--------------------+--------------------+--------------------+--------------------+---------------+--------------+--------------+--------------+---------------+\n",
      "|cmp_fname_c1|cmp_fname_c2|cmp_lname_c1|cmp_lname_c2|cmp_sex|cmp_bd|cmp_bm|cmp_by|cmp_plz|is_match|cmp_fname_c1_imputed|cmp_fname_c2_imputed|cmp_lname_c1_imputed|cmp_lname_c2_imputed|cmp_sex_imputed|cmp_bd_imputed|cmp_bm_imputed|cmp_by_imputed|cmp_plz_imputed|\n",
      "+------------+------------+------------+------------+-------+------+------+------+-------+--------+--------------------+--------------------+--------------------+--------------------+---------------+--------------+--------------+--------------+---------------+\n",
      "|   0.8333333|        null|         1.0|        null|      1|     1|     1|     1|      0|    true|           0.8333333|           0.9000177|                 1.0|          0.31841284|              1|             1|             1|             1|              0|\n",
      "|         1.0|        null|         1.0|        null|      1|     1|     1|     1|      1|    true|                 1.0|           0.9000177|                 1.0|          0.31841284|              1|             1|             1|             1|              1|\n",
      "|         1.0|        null|         1.0|        null|      1|     1|     1|     1|      1|    true|                 1.0|           0.9000177|                 1.0|          0.31841284|              1|             1|             1|             1|              1|\n",
      "|         1.0|        null|         1.0|        null|      1|     1|     1|     1|      1|    true|                 1.0|           0.9000177|                 1.0|          0.31841284|              1|             1|             1|             1|              1|\n",
      "|         1.0|        null|         1.0|         1.0|      1|     1|     1|     1|      1|    true|                 1.0|           0.9000177|                 1.0|                 1.0|              1|             1|             1|             1|              1|\n",
      "|         1.0|        null|         1.0|        null|      1|     1|     1|     1|      1|    true|                 1.0|           0.9000177|                 1.0|          0.31841284|              1|             1|             1|             1|              1|\n",
      "|         1.0|        null|         1.0|        null|      1|     1|     1|     1|      1|    true|                 1.0|           0.9000177|                 1.0|          0.31841284|              1|             1|             1|             1|              1|\n",
      "|         1.0|        null|         1.0|        null|      1|     1|     1|     1|      0|    true|                 1.0|           0.9000177|                 1.0|          0.31841284|              1|             1|             1|             1|              0|\n",
      "|         1.0|        null|         1.0|        null|      1|     1|     1|     1|      1|    true|                 1.0|           0.9000177|                 1.0|          0.31841284|              1|             1|             1|             1|              1|\n",
      "|         1.0|        null|         1.0|        null|      1|     1|     1|     1|      1|    true|                 1.0|           0.9000177|                 1.0|          0.31841284|              1|             1|             1|             1|              1|\n",
      "|         1.0|        null|         1.0|        null|      1|     1|     1|     1|      1|    true|                 1.0|           0.9000177|                 1.0|          0.31841284|              1|             1|             1|             1|              1|\n",
      "|         1.0|        null|         1.0|        null|      1|     1|     1|     1|      1|    true|                 1.0|           0.9000177|                 1.0|          0.31841284|              1|             1|             1|             1|              1|\n",
      "|         1.0|        null|         1.0|        null|      1|     1|     1|     1|      1|    true|                 1.0|           0.9000177|                 1.0|          0.31841284|              1|             1|             1|             1|              1|\n",
      "|         1.0|        null|         1.0|        null|      1|     1|     1|     1|      1|    true|                 1.0|           0.9000177|                 1.0|          0.31841284|              1|             1|             1|             1|              1|\n",
      "|         1.0|        null|         1.0|        null|      1|     0|     1|     1|      1|    true|                 1.0|           0.9000177|                 1.0|          0.31841284|              1|             0|             1|             1|              1|\n",
      "|         1.0|        null|         1.0|        null|      1|     1|     1|     1|      1|    true|                 1.0|           0.9000177|                 1.0|          0.31841284|              1|             1|             1|             1|              1|\n",
      "|         1.0|         1.0|         1.0|        null|      1|     1|     1|     1|      1|    true|                 1.0|                 1.0|                 1.0|          0.31841284|              1|             1|             1|             1|              1|\n",
      "|         1.0|         1.0|         1.0|        null|      1|     1|     1|     1|      1|    true|                 1.0|                 1.0|                 1.0|          0.31841284|              1|             1|             1|             1|              1|\n",
      "|         1.0|        null|         1.0|        null|      1|     1|     1|     1|      1|    true|                 1.0|           0.9000177|                 1.0|          0.31841284|              1|             1|             1|             1|              1|\n",
      "|         1.0|        null|         1.0|        null|      1|     1|     1|     1|      1|    true|                 1.0|           0.9000177|                 1.0|          0.31841284|              1|             1|             1|             1|              1|\n",
      "+------------+------------+------------+------------+-------+------+------+------+-------+--------+--------------------+--------------------+--------------------+--------------------+---------------+--------------+--------------+--------------+---------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "final_imputed_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['cmp_fname_c1',\n",
       " 'cmp_fname_c2',\n",
       " 'cmp_lname_c1',\n",
       " 'cmp_lname_c2',\n",
       " 'cmp_sex',\n",
       " 'cmp_bd',\n",
       " 'cmp_bm',\n",
       " 'cmp_by',\n",
       " 'cmp_plz',\n",
       " 'is_match',\n",
       " 'cmp_fname_c1_imputed',\n",
       " 'cmp_fname_c2_imputed',\n",
       " 'cmp_lname_c1_imputed',\n",
       " 'cmp_lname_c2_imputed',\n",
       " 'cmp_sex_imputed',\n",
       " 'cmp_bd_imputed',\n",
       " 'cmp_bm_imputed',\n",
       " 'cmp_by_imputed',\n",
       " 'cmp_plz_imputed']"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "final_imputed_df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "prep_df=final_imputed_df.select([x for x in final_imputed_df.columns if '_imputed' in x or x=='is_match'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+--------------------+--------------------+--------------------+--------------------+------------------+-------------------+-------------------+-------------------+-------------------+\n",
      "|summary|cmp_fname_c1_imputed|cmp_fname_c2_imputed|cmp_lname_c1_imputed|cmp_lname_c2_imputed|   cmp_sex_imputed|     cmp_bd_imputed|     cmp_bm_imputed|     cmp_by_imputed|    cmp_plz_imputed|\n",
      "+-------+--------------------+--------------------+--------------------+--------------------+------------------+-------------------+-------------------+-------------------+-------------------+\n",
      "|  count|             5749132|             5749132|             5749132|             5749132|           5749132|            5749132|            5749132|            5749132|            5749132|\n",
      "|   mean|  0.7129024717725752|  0.9000176786244445|  0.3156278224158526| 0.31841284036351114| 0.955001381078048|0.22443422763645016| 0.4887876987343481|0.22271779461664823|0.00551631098398854|\n",
      "| stddev| 0.38872431001387875|  0.0364384567674084|  0.3342336344123867|  0.0076286489245731|0.2073011111689783|0.41720922254687776|0.49987431196581056|0.41607043717281594|0.07406674187060545|\n",
      "|    min|                 0.0|                 0.0|                 0.0|                 0.0|                 0|                  0|                  0|                  0|                  0|\n",
      "|    max|                 1.0|                 1.0|                 1.0|                 1.0|                 1|                  1|                  1|                  1|                  1|\n",
      "+-------+--------------------+--------------------+--------------------+--------------------+------------------+-------------------+-------------------+-------------------+-------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "prep_df.describe().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['is_match',\n",
       " 'cmp_fname_c1_imputed',\n",
       " 'cmp_fname_c2_imputed',\n",
       " 'cmp_lname_c1_imputed',\n",
       " 'cmp_lname_c2_imputed',\n",
       " 'cmp_sex_imputed',\n",
       " 'cmp_bd_imputed',\n",
       " 'cmp_bm_imputed',\n",
       " 'cmp_by_imputed',\n",
       " 'cmp_plz_imputed']"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prep_df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.feature import FeatureHasher,OneHotEncoder,VectorAssembler,StringIndexer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+--------------------+--------------------+--------------------+--------------------+---------------+--------------+--------------+--------------+---------------+-----+\n",
      "|is_match|cmp_fname_c1_imputed|cmp_fname_c2_imputed|cmp_lname_c1_imputed|cmp_lname_c2_imputed|cmp_sex_imputed|cmp_bd_imputed|cmp_bm_imputed|cmp_by_imputed|cmp_plz_imputed|label|\n",
      "+--------+--------------------+--------------------+--------------------+--------------------+---------------+--------------+--------------+--------------+---------------+-----+\n",
      "|true    |0.8333333           |0.9000177           |1.0                 |0.31841284          |1              |1             |1             |1             |0              |1    |\n",
      "|true    |1.0                 |0.9000177           |1.0                 |0.31841284          |1              |1             |1             |1             |1              |1    |\n",
      "|true    |1.0                 |0.9000177           |1.0                 |0.31841284          |1              |1             |1             |1             |1              |1    |\n",
      "+--------+--------------------+--------------------+--------------------+--------------------+---------------+--------------+--------------+--------------+---------------+-----+\n",
      "only showing top 3 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "prep_df.withColumn('label',prep_df['is_match'].cast('integer')).show(3,truncate=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import when, lit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "rep_df = prep_df.withColumn('label', \\\n",
    "                   when(df['is_match']==True, \n",
    "    lit(1)).otherwise(0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+--------------------+--------------------+--------------------+--------------------+---------------+--------------+--------------+--------------+---------------+-----+\n",
      "|is_match|cmp_fname_c1_imputed|cmp_fname_c2_imputed|cmp_lname_c1_imputed|cmp_lname_c2_imputed|cmp_sex_imputed|cmp_bd_imputed|cmp_bm_imputed|cmp_by_imputed|cmp_plz_imputed|label|\n",
      "+--------+--------------------+--------------------+--------------------+--------------------+---------------+--------------+--------------+--------------+---------------+-----+\n",
      "|true    |0.8333333           |0.9000177           |1.0                 |0.31841284          |1              |1             |1             |1             |0              |1    |\n",
      "+--------+--------------------+--------------------+--------------------+--------------------+---------------+--------------+--------------+--------------+---------------+-----+\n",
      "only showing top 1 row\n",
      "\n"
     ]
    }
   ],
   "source": [
    "rep_df.show(1,truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+--------------------+--------------------+--------------------+--------------------+---------------+--------------+--------------+--------------+---------------+-----+\n",
      "|is_match|cmp_fname_c1_imputed|cmp_fname_c2_imputed|cmp_lname_c1_imputed|cmp_lname_c2_imputed|cmp_sex_imputed|cmp_bd_imputed|cmp_bm_imputed|cmp_by_imputed|cmp_plz_imputed|label|\n",
      "+--------+--------------------+--------------------+--------------------+--------------------+---------------+--------------+--------------+--------------+---------------+-----+\n",
      "|true    |0.8333333           |0.9000177           |1.0                 |0.31841284          |1              |1             |1             |1             |0              |1    |\n",
      "|true    |1.0                 |0.9000177           |1.0                 |0.31841284          |1              |1             |1             |1             |1              |1    |\n",
      "|true    |1.0                 |0.9000177           |1.0                 |0.31841284          |1              |1             |1             |1             |1              |1    |\n",
      "+--------+--------------------+--------------------+--------------------+--------------------+---------------+--------------+--------------+--------------+---------------+-----+\n",
      "only showing top 3 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "rep_df[rep_df['label']>0].show(3,truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------------------------------------------------------------------------------+-----+\n",
      "|features                                                                          |label|\n",
      "+----------------------------------------------------------------------------------+-----+\n",
      "|[0.9000176787376404,1.0,1.0,0.0,0.3184128403663635,1.0,1.0,0.8333333134651184,1.0]|1    |\n",
      "|[0.9000176787376404,1.0,1.0,1.0,0.3184128403663635,1.0,1.0,1.0,1.0]               |1    |\n",
      "|[0.9000176787376404,1.0,1.0,1.0,0.3184128403663635,1.0,1.0,1.0,1.0]               |1    |\n",
      "|[0.9000176787376404,1.0,1.0,1.0,0.3184128403663635,1.0,1.0,1.0,1.0]               |1    |\n",
      "|[0.9000176787376404,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0]                              |1    |\n",
      "+----------------------------------------------------------------------------------+-----+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "features=set(rep_df.columns) - set(['label','is_match'])\n",
    "assembler = VectorAssembler(inputCols=list(features),\n",
    "                             outputCol='features')\n",
    "\n",
    "# Consolidate predictor columns\n",
    "assembled_df = assembler.transform(rep_df)\n",
    "assembled_df.select('features', 'label').show(5, truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4025517"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train, test = assembled_df.randomSplit([0.7, 0.3], seed=42)\n",
    "train.count()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ML Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.mllib.evaluation import BinaryClassificationMetrics\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+----------+-----------------------------------------+\n",
      "|label|prediction|probability                              |\n",
      "+-----+----------+-----------------------------------------+\n",
      "|0    |0.0       |[0.9999965769719209,3.423028079099333E-6]|\n",
      "|0    |0.0       |[0.9999965769719209,3.423028079099333E-6]|\n",
      "|0    |0.0       |[0.9999965769719209,3.423028079099333E-6]|\n",
      "|0    |0.0       |[0.9999965769719209,3.423028079099333E-6]|\n",
      "|0    |0.0       |[0.9999965769719209,3.423028079099333E-6]|\n",
      "+-----+----------+-----------------------------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.ml.classification import DecisionTreeClassifier\n",
    "\n",
    "# Create a classifier object and fit to the training data\n",
    "tree = DecisionTreeClassifier()\n",
    "tree_model = tree.fit(train)\n",
    "\n",
    "# Create predictions for the testing data and take a look at the predictions\n",
    "prediction = tree_model.transform(test)\n",
    "prediction.select('label', 'prediction', 'probability').show(5, False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def evaluate(pred):\n",
    "    pred.groupBy('label', 'prediction').count().show()\n",
    "\n",
    "    # Calculate the elements of the confusion matrix\n",
    "    TN = pred.filter('prediction = 0 AND label = prediction').count()\n",
    "    TP = pred.filter('prediction = 1 AND label = prediction').count()\n",
    "    FN = pred.filter('prediction = 0 AND label = 1').count()\n",
    "    FP = pred.filter('prediction = 1 AND label = 0').count()\n",
    "\n",
    "    # Accuracy measures the proportion of correct predictions\n",
    "    accuracy = (TN + TP) / (TN + TP + FN + FP)\n",
    "    recall = (TP) / (TP+FN)\n",
    "    precision= (TP) / (TP+FP)\n",
    "    f1=2*(precision*recall)/(precision+recall)\n",
    "    print('EVALUATION SUMMARY:')\n",
    "    print(f'accuracy:{accuracy} \\nprecision:{precision} \\nrecall:{recall} \\nf1-score:{f1}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+----------+-------+\n",
      "|label|prediction|  count|\n",
      "+-----+----------+-------+\n",
      "|    1|       0.0|     33|\n",
      "|    0|       0.0|1717323|\n",
      "|    1|       1.0|   6166|\n",
      "|    0|       1.0|     93|\n",
      "+-----+----------+-------+\n",
      "\n",
      "EVALUATION SUMMARY:\n",
      "accuracy:0.9999268978281113 \n",
      "precision:0.9851413963891995 \n",
      "recall:0.9946765607356025 \n",
      "f1-score:0.9898860170171776\n"
     ]
    }
   ],
   "source": [
    "evaluate(prediction)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+----------+-------------------------------------------+\n",
      "|label|prediction|probability                                |\n",
      "+-----+----------+-------------------------------------------+\n",
      "|0    |0.0       |[0.9999999999783138,2.1686208384608108E-11]|\n",
      "|0    |0.0       |[0.9999999999262756,7.372435995023352E-11] |\n",
      "|0    |0.0       |[0.9999999997835478,2.1645218950538947E-10]|\n",
      "|0    |0.0       |[1.0,0.0]                                  |\n",
      "|0    |0.0       |[0.9999999999999998,2.220446049250313E-16] |\n",
      "+-----+----------+-------------------------------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.ml.classification import LogisticRegression\n",
    "from pyspark.ml import Pipeline\n",
    "logistic = LogisticRegression()\n",
    "lr_pipeline = Pipeline(stages=[ logistic])\n",
    "lr_model=lr_pipeline.fit(train)\n",
    "lr_prediction=lr_model.transform(test)\n",
    "lr_prediction.select('label', 'prediction', 'probability').show(5, False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+----------+-------+\n",
      "|label|prediction|  count|\n",
      "+-----+----------+-------+\n",
      "|    0|       0.0|1717405|\n",
      "|    1|       1.0|   6181|\n",
      "|    0|       1.0|     11|\n",
      "|    1|       0.0|     18|\n",
      "+-----+----------+-------+\n",
      "\n",
      "EVALUATION SUMMARY:\n",
      "accuracy:0.9999831748969462 \n",
      "precision:0.9982235142118863 \n",
      "recall:0.9970963058557832 \n",
      "f1-score:0.9976595916390929\n"
     ]
    }
   ],
   "source": [
    "evaluate(lr_prediction)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+----------+------------------------------------------+\n",
      "|label|prediction|probability                               |\n",
      "+-----+----------+------------------------------------------+\n",
      "|0    |0.0       |[0.9999644889923797,3.5511007620311594E-5]|\n",
      "|0    |0.0       |[0.9999720909893208,2.790901067927771E-5] |\n",
      "|0    |0.0       |[0.9999644889923797,3.5511007620311594E-5]|\n",
      "|0    |0.0       |[0.9999838082776951,1.6191722304915428E-5]|\n",
      "|0    |0.0       |[0.9999838082776951,1.6191722304915428E-5]|\n",
      "+-----+----------+------------------------------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.ml.classification import RandomForestClassifier\n",
    "from pyspark.ml import Pipeline\n",
    "rf = RandomForestClassifier(labelCol=\"label\", featuresCol=\"features\")\n",
    "pipeline = Pipeline(stages=[ rf])\n",
    "rf_model=pipeline.fit(train)\n",
    "rf_predictions=rf_model.transform(test)\n",
    "rf_predictions.select('label', 'prediction', 'probability').show(5, False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['is_match',\n",
       " 'cmp_fname_c1_imputed',\n",
       " 'cmp_fname_c2_imputed',\n",
       " 'cmp_lname_c1_imputed',\n",
       " 'cmp_lname_c2_imputed',\n",
       " 'cmp_sex_imputed',\n",
       " 'cmp_bd_imputed',\n",
       " 'cmp_bm_imputed',\n",
       " 'cmp_by_imputed',\n",
       " 'cmp_plz_imputed',\n",
       " 'label',\n",
       " 'features',\n",
       " 'rawPrediction',\n",
       " 'probability',\n",
       " 'prediction']"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rf_predictions.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+----------+-------+\n",
      "|label|prediction|  count|\n",
      "+-----+----------+-------+\n",
      "|    1|       0.0|     73|\n",
      "|    0|       0.0|1717414|\n",
      "|    1|       1.0|   6126|\n",
      "|    0|       1.0|      2|\n",
      "+-----+----------+-------+\n",
      "\n",
      "EVALUATION SUMMARY:\n",
      "accuracy:0.9999564868024472 \n",
      "precision:0.9996736292428199 \n",
      "recall:0.9882239070817874 \n",
      "f1-score:0.9939157945972257\n"
     ]
    }
   ],
   "source": [
    "evaluate(rf_predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# todo: implement area under ROC \n",
    "\n",
    "# predictionAndLabels = prediction.select('prediction','label').rdd \\\n",
    "#                             .map(lambda x: (x[0],x[1]))\n",
    "\n",
    "\n",
    "# # Instantiate metrics object\n",
    "# metrics = BinaryClassificationMetrics(predictionAndLabels)\n",
    "# # Area under ROC curve\n",
    "# print(\"Area under ROC = %s\" % metrics.areaUnderROC)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cross Validation "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.tuning import CrossValidator, ParamGridBuilder, CrossValidatorModel\n",
    "from pyspark.ml.evaluation import BinaryClassificationEvaluator\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr_cs = LogisticRegression()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.classification import LogisticRegression\n",
    "lr_cs = LogisticRegression()\n",
    "grid = ParamGridBuilder().addGrid(lr_cs.regParam, [0.1, 0.01]) \\\n",
    "    .addGrid(lr_cs.fitIntercept, [False, True])\\\n",
    "    .addGrid(lr_cs.elasticNetParam, [0.0, 1.0])\\\n",
    "    .build()\n",
    "evaluator = BinaryClassificationEvaluator()\n",
    "cv = CrossValidator(estimator=lr_cs, estimatorParamMaps=grid, evaluator=evaluator)\n",
    "cvModel = cv.fit(train)\n",
    "lrprediction=cvModel.transform(test)\n",
    "\n",
    "\n",
    "\n",
    "print('Accuracy:', evaluator.evaluate(lrprediction))\n",
    "print('AUC:', BinaryClassificationMetrics(lrprediction['label','prediction'].rdd).areaUnderROC)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluate(lrprediction)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.classification import DecisionTreeClassifier\n",
    "\n",
    "dt = DecisionTreeClassifier()\n",
    "grid = ParamGridBuilder() \\\n",
    "        .addGrid(dt.maxDepth,  [2, 5, 10, 20, 30]) \\\n",
    "        .addGrid(dt.maxBins,  [10, 20, 40, 80, 100]) \\\n",
    "        .build()\n",
    "dtevaluator = BinaryClassificationEvaluator(rawPredictionCol=\"rawPrediction\")\n",
    "cv = CrossValidator(estimator=dt, \n",
    "                    estimatorParamMaps=grid, \n",
    "                    evaluator=dtevaluator,\n",
    "                    numFolds = 3)\n",
    "dtModel = cv.fit(train)\n",
    "dtpredictions = dtModel.transform(test)\n",
    "\n",
    "print('Accuracy:', dtevaluator.evaluate(dtpredictions))\n",
    "print('AUC:', BinaryClassificationMetrics(dtpredictions['label','prediction'].rdd).areaUnderROC)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluate(dtpredictions)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "6ed0a4610ee5d358854c63a9c9df0be609d14f26b47cd9f541e7269cbaa1b618"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
