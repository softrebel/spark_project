{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark import SparkConf, SparkContext\n",
    "from pyspark.sql.types import StructType,IntegerType,FloatType,BooleanType\n",
    "conf = SparkConf().setMaster(\"local[*]\").setAppName(\"My App\")\n",
    "sc = SparkContext.getOrCreate(conf = conf)\n",
    "spark=SparkSession.builder.appName('myApp').getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data(files,schema):\n",
    "    df=spark.read.csv(files,header=True\n",
    "                  ,schema=schema)\n",
    "    return df\n",
    "\n",
    "def load_record_linkage_data():\n",
    "    schema = StructType() \\\n",
    "      .add(\"id_1\",IntegerType(),True) \\\n",
    "      .add(\"id_2\",IntegerType(),True) \\\n",
    "      .add(\"cmp_fname_c1\",FloatType(),True) \\\n",
    "      .add(\"cmp_fname_c2\",FloatType(),True) \\\n",
    "      .add(\"cmp_lname_c1\",FloatType(),True) \\\n",
    "      .add(\"cmp_lname_c2\",FloatType(),True) \\\n",
    "      .add(\"cmp_sex\",IntegerType(),True) \\\n",
    "      .add(\"cmp_bd\",IntegerType(),True) \\\n",
    "      .add(\"cmp_bm\",IntegerType(),True) \\\n",
    "      .add(\"cmp_by\",IntegerType(),True) \\\n",
    "      .add(\"cmp_plz\",IntegerType(),True) \\\n",
    "      .add(\"is_match\",BooleanType(),False)\n",
    "    files=[f'./data/block_{id}.csv' for id in range(1,11)]\n",
    "    return load_data(files,schema=schema)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "df=load_record_linkage_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+-------+\n",
      "|cmp_plz|  count|\n",
      "+-------+-------+\n",
      "|   null|  12843|\n",
      "|      1|  31714|\n",
      "|      0|5704575|\n",
      "+-------+-------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.groupBy('cmp_plz').count().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5749132"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- id_1: integer (nullable = true)\n",
      " |-- id_2: integer (nullable = true)\n",
      " |-- cmp_fname_c1: float (nullable = true)\n",
      " |-- cmp_fname_c2: float (nullable = true)\n",
      " |-- cmp_lname_c1: float (nullable = true)\n",
      " |-- cmp_lname_c2: float (nullable = true)\n",
      " |-- cmp_sex: integer (nullable = true)\n",
      " |-- cmp_bd: integer (nullable = true)\n",
      " |-- cmp_bm: integer (nullable = true)\n",
      " |-- cmp_by: integer (nullable = true)\n",
      " |-- cmp_plz: integer (nullable = true)\n",
      " |-- is_match: boolean (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.printSchema()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Missing Values"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Drop Missing Values\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "miss_df=df.drop('id_1','id_2')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "اگر همه داده های گم شده را حذف کنیم، کلا 20 رکورد باقی می‌ ماند !\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "20"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "miss_df.replace('?',None).na.drop().count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "miss_df=miss_df.replace('?',None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(cmp_fname_c1=0.8333333134651184, cmp_fname_c2=None, cmp_lname_c1=1.0, cmp_lname_c2=None, cmp_sex=1, cmp_bd=1, cmp_bm=1, cmp_by=1, cmp_plz=0, is_match=True),\n",
       " Row(cmp_fname_c1=1.0, cmp_fname_c2=None, cmp_lname_c1=1.0, cmp_lname_c2=None, cmp_sex=1, cmp_bd=1, cmp_bm=1, cmp_by=1, cmp_plz=1, is_match=True),\n",
       " Row(cmp_fname_c1=1.0, cmp_fname_c2=None, cmp_lname_c1=1.0, cmp_lname_c2=None, cmp_sex=1, cmp_bd=1, cmp_bm=1, cmp_by=1, cmp_plz=1, is_match=True)]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "miss_df.head(3)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "هیچ رکوردی که همه یا حداقل 2 تا از متغیرهای آن گم شده باشد، وجود ندارد"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5749132"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "miss_df.na.drop(how='all').count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5749132"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "miss_df.na.drop(how='any',thresh=2).count()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fill the Missing Values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.feature import Imputer\n",
    "from pyspark.sql.functions import when, lit\n",
    "# for float variables\n",
    "\n",
    "\n",
    "\n",
    "def convert_label_binary(input_df):\n",
    "    temp = input_df.withColumn('label',\n",
    "                             when(input_df['is_match']==True,\n",
    "                                  lit(1)).otherwise(0)\n",
    "                                  ) \n",
    "    return temp\n",
    "\n",
    "def fill_missing_values(input_df):\n",
    "    miss_df=input_df.drop('id_1','id_2')\n",
    "    miss_df=miss_df.replace('?',None)\n",
    "    float_cols=[\n",
    "    'cmp_fname_c1', \n",
    "    'cmp_fname_c2', \n",
    "    'cmp_lname_c1', \n",
    "    'cmp_lname_c2', \n",
    "    ]\n",
    "    float_imputer = Imputer(\n",
    "        inputCols=float_cols,\n",
    "        outputCols=[f\"{col}_imputed\" for col in float_cols]\n",
    "    ).setStrategy('mean')\n",
    "\n",
    "    # for binary variables\n",
    "    binary_cols=[\n",
    "        'cmp_sex', \n",
    "        'cmp_bd', \n",
    "        'cmp_bm', \n",
    "        'cmp_by',\n",
    "        'cmp_plz',\n",
    "    ]\n",
    "    binary_imputer = Imputer(\n",
    "        inputCols=binary_cols,\n",
    "        outputCols=[f\"{col}_imputed\" for col in binary_cols]\n",
    "    ).setStrategy('mode')\n",
    "    imputed_df=float_imputer.fit(miss_df).transform(miss_df)\n",
    "    output_df=binary_imputer.fit(imputed_df).transform(imputed_df)\n",
    "    output_df=output_df.select([x for x in output_df.columns if '_imputed' in x or x=='is_match'])\n",
    "    return output_df\n",
    "\n",
    "\n",
    "def preprocessing_df(input_df):\n",
    "    return convert_label_binary(fill_missing_values(input_df))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "prep_df=preprocessing_df(df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+-------+\n",
      "|cmp_fname_c1_imputed|  count|\n",
      "+--------------------+-------+\n",
      "|          0.27272728|    454|\n",
      "|           0.8181818|      3|\n",
      "|          0.16666667| 152732|\n",
      "|                0.25| 137039|\n",
      "|               0.875|  71211|\n",
      "|           0.5714286|   7414|\n",
      "|          0.47058824|     11|\n",
      "|                0.75|  46521|\n",
      "|                 0.1|  10357|\n",
      "|          0.11111111| 123127|\n",
      "|               0.125| 155172|\n",
      "|          0.36363637|    293|\n",
      "|           0.7777778|   3083|\n",
      "|                 0.6|  19725|\n",
      "|                 0.9|   7780|\n",
      "|                 0.5|  44615|\n",
      "|          0.42857143|  34463|\n",
      "|           0.2857143|  78429|\n",
      "|          0.33333334|  94936|\n",
      "|                 1.0|3508203|\n",
      "+--------------------+-------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "prep_df.groupBy('cmp_fname_c1_imputed').count().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+--------------------+--------------------+--------------------+--------------------+---------------+--------------+--------------+--------------+---------------+-----+\n",
      "|is_match|cmp_fname_c1_imputed|cmp_fname_c2_imputed|cmp_lname_c1_imputed|cmp_lname_c2_imputed|cmp_sex_imputed|cmp_bd_imputed|cmp_bm_imputed|cmp_by_imputed|cmp_plz_imputed|label|\n",
      "+--------+--------------------+--------------------+--------------------+--------------------+---------------+--------------+--------------+--------------+---------------+-----+\n",
      "|true    |0.8333333           |0.9000177           |1.0                 |0.31841284          |1              |1             |1             |1             |0              |1    |\n",
      "+--------+--------------------+--------------------+--------------------+--------------------+---------------+--------------+--------------+--------------+---------------+-----+\n",
      "only showing top 1 row\n",
      "\n"
     ]
    }
   ],
   "source": [
    "prep_df.show(1,truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+--------------------+--------------------+--------------------+--------------------+---------------+--------------+--------------+--------------+---------------+-----+\n",
      "|is_match|cmp_fname_c1_imputed|cmp_fname_c2_imputed|cmp_lname_c1_imputed|cmp_lname_c2_imputed|cmp_sex_imputed|cmp_bd_imputed|cmp_bm_imputed|cmp_by_imputed|cmp_plz_imputed|label|\n",
      "+--------+--------------------+--------------------+--------------------+--------------------+---------------+--------------+--------------+--------------+---------------+-----+\n",
      "|true    |0.8333333           |0.9000177           |1.0                 |0.31841284          |1              |1             |1             |1             |0              |1    |\n",
      "|true    |1.0                 |0.9000177           |1.0                 |0.31841284          |1              |1             |1             |1             |1              |1    |\n",
      "|true    |1.0                 |0.9000177           |1.0                 |0.31841284          |1              |1             |1             |1             |1              |1    |\n",
      "+--------+--------------------+--------------------+--------------------+--------------------+---------------+--------------+--------------+--------------+---------------+-----+\n",
      "only showing top 3 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "prep_df[prep_df['label']>0].show(3,truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.feature import FeatureHasher,OneHotEncoder,VectorAssembler,StringIndexer\n",
    "def feature_engineering(input_df,feature_list,label_name):\n",
    "    assembler = VectorAssembler(inputCols=feature_list,\n",
    "                             outputCol='features')\n",
    "    assembled_df = assembler.transform(input_df)\n",
    "    output_df=assembled_df.select('features', label_name)\n",
    "    return output_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------------------------------------------------------------------------------+-----+\n",
      "|features                                                                          |label|\n",
      "+----------------------------------------------------------------------------------+-----+\n",
      "|[0.0,0.8333333134651184,1.0,1.0,1.0,1.0,1.0,0.9000176787376404,0.3184128403663635]|1    |\n",
      "|[1.0,1.0,1.0,1.0,1.0,1.0,1.0,0.9000176787376404,0.3184128403663635]               |1    |\n",
      "|[1.0,1.0,1.0,1.0,1.0,1.0,1.0,0.9000176787376404,0.3184128403663635]               |1    |\n",
      "|[1.0,1.0,1.0,1.0,1.0,1.0,1.0,0.9000176787376404,0.3184128403663635]               |1    |\n",
      "|[1.0,1.0,1.0,1.0,1.0,1.0,1.0,0.9000176787376404,1.0]                              |1    |\n",
      "+----------------------------------------------------------------------------------+-----+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "features=list(set(prep_df.columns) - set(['label','is_match']))\n",
    "\n",
    "assembled_df = feature_engineering(prep_df,features,'label')\n",
    "assembled_df.show(5, truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_train_split(input_df,train_size=0.7):\n",
    "    train, test = assembled_df.randomSplit([train_size,1 - train_size], seed=42)\n",
    "    return train,test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4025517"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train, test = test_train_split(assembled_df,0.7)\n",
    "train.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+-------+\n",
      "|label|  count|\n",
      "+-----+-------+\n",
      "|    1|  20931|\n",
      "|    0|5728201|\n",
      "+-----+-------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "prep_df.groupBy('label').count().show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ML Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.mllib.evaluation import BinaryClassificationMetrics\n",
    "from pyspark.ml.evaluation import BinaryClassificationEvaluator,MulticlassClassificationEvaluator\n",
    "from pyspark.ml.classification import DecisionTreeClassifier,LogisticRegression\n",
    "from pyspark.ml import Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def evaluate_from_scratch(pred):\n",
    "    pred.groupBy('label', 'prediction').count().show()\n",
    "\n",
    "    # Calculate the elements of the confusion matrix\n",
    "    TN = pred.filter('prediction = 0 AND label = prediction').count()\n",
    "    TP = pred.filter('prediction = 1 AND label = prediction').count()\n",
    "    FN = pred.filter('prediction = 0 AND label = 1').count()\n",
    "    FP = pred.filter('prediction = 1 AND label = 0').count()\n",
    "\n",
    "    # Accuracy measures the proportion of correct predictions\n",
    "    accuracy = (TN + TP) / (TN + TP + FN + FP)\n",
    "    recall = (TP) / (TP+FN)\n",
    "    precision= (TP) / (TP+FP)\n",
    "    f1=2*(precision*recall)/(precision+recall)\n",
    "    print('EVALUATION SUMMARY:')\n",
    "    print(f'accuracy:{accuracy} \\nprecision:{precision} \\nrecall:{recall} \\nf1-score:{f1}')\n",
    "\n",
    "def evaluate_from_spark(predictions):\n",
    "    lr_eval = BinaryClassificationEvaluator(rawPredictionCol=\"probability\", labelCol=\"label\")\n",
    "    lr_eval2= MulticlassClassificationEvaluator(predictionCol=\"prediction\", labelCol=\"label\")\n",
    "    lr_AUC  = lr_eval.evaluate(predictions)\n",
    "    lr_ACC  = lr_eval2.evaluate(predictions, {lr_eval2.metricName:\"accuracy\"})\n",
    "    lr_PREC  = lr_eval2.evaluate(predictions, {lr_eval2.metricName:\"weightedPrecision\"})\n",
    "    lr_REC  = lr_eval2.evaluate(predictions, {lr_eval2.metricName:\"weightedRecall\"})\n",
    "    lr_F1  = lr_eval2.evaluate(predictions, {lr_eval2.metricName:\"f1\"})\n",
    "    print(\"Logistic Regression Performance Measure\")\n",
    "    print(\"Accuracy = %0.8f\" % lr_ACC)\n",
    "    print(\"Weighted Precision = %0.8f\" % lr_PREC)\n",
    "    print(\"Weighted Recall = %0.8f\" % lr_REC)\n",
    "    print(\"F1 = %0.8f\" % lr_F1)\n",
    "\n",
    "    print(\"AUC = %.8f\" % lr_AUC)\n",
    "\n",
    "\n",
    "def evaluate(predictions):\n",
    "    print('Evaluate From Scratch:')\n",
    "    evaluate_from_scratch(predictions)\n",
    "    print('Evaluate From Spark Library:')\n",
    "    evaluate_from_spark(predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr=LogisticRegression(featuresCol='features', labelCol='label')\n",
    "lr_model = lr.fit(train)\n",
    "lr_result = lr_model.transform(test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+----------+--------------------+\n",
      "|label|prediction|         probability|\n",
      "+-----+----------+--------------------+\n",
      "|    0|       0.0|[0.99999999999977...|\n",
      "|    0|       0.0|[0.99999999999864...|\n",
      "|    0|       0.0|[0.99999999999864...|\n",
      "+-----+----------+--------------------+\n",
      "only showing top 3 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "lr_result.select('label', 'prediction', 'probability').show(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluate From Scratch:\n",
      "+-----+----------+-------+\n",
      "|label|prediction|  count|\n",
      "+-----+----------+-------+\n",
      "|    1|       0.0|     12|\n",
      "|    0|       0.0|1717400|\n",
      "|    1|       1.0|   6200|\n",
      "|    0|       1.0|      3|\n",
      "+-----+----------+-------+\n",
      "\n",
      "EVALUATION SUMMARY:\n",
      "accuracy:0.9999912973604894 \n",
      "precision:0.999516363050137 \n",
      "recall:0.9980682549903412 \n",
      "f1-score:0.9987917841320983\n",
      "Evaluate From Spark Library:\n",
      "Logistic Regression Performance Measure\n",
      "Accuracy = 0.99999130\n",
      "Weighted Precision = 0.99999129\n",
      "Weighted Recall = 0.99999130\n",
      "F1 = 0.99999129\n",
      "AUC = 0.99999988\n"
     ]
    }
   ],
   "source": [
    "evaluate(lr_result)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Not Refactored yet...\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Exception ignored in: <function JavaWrapper.__del__ at 0x00000200FB2459D0>\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\wamp64\\www\\spark_project\\venv\\lib\\site-packages\\pyspark\\ml\\wrapper.py\", line 53, in __del__\n",
      "    if SparkContext._active_spark_context and self._java_obj is not None:\n",
      "AttributeError: 'BinaryClassificationEvaluator' object has no attribute '_java_obj'\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logistic Regression Performance Measure\n",
      "Accuracy = 0.99999130\n",
      "Weighted Precision = 0.99999129\n",
      "Weighted Recall = 0.99999130\n",
      "F1 = 0.99999129\n",
      "AUC = 0.99999988\n",
      "\n",
      "Evaluation:\n",
      "+-----+----------+-------+\n",
      "|label|prediction|  count|\n",
      "+-----+----------+-------+\n",
      "|    1|       0.0|     12|\n",
      "|    0|       0.0|1717400|\n",
      "|    1|       1.0|   6200|\n",
      "|    0|       1.0|      3|\n",
      "+-----+----------+-------+\n",
      "\n",
      "EVALUATION SUMMARY:\n",
      "accuracy:0.9999912973604894 \n",
      "precision:0.999516363050137 \n",
      "recall:0.9980682549903412 \n",
      "f1-score:0.9987917841320983\n"
     ]
    }
   ],
   "source": [
    "lr=LogisticRegression(featuresCol='features', labelCol='label')\n",
    "pipeline = Pipeline(stages=[lr])\n",
    "model = pipeline.fit(train)\n",
    "predictions = model.transform(test)\n",
    "\n",
    "#Evaluate model by checking accuracy and AUC value\n",
    "lr_eval = BinaryClassificationEvaluator(rawPredictionCol=\"probability\", labelCol=\"label\")\n",
    "lr_eval2= MulticlassClassificationEvaluator(predictionCol=\"prediction\", labelCol=\"label\")\n",
    "lr_AUC  = lr_eval.evaluate(predictions)\n",
    "lr_ACC  = lr_eval2.evaluate(predictions, {lr_eval2.metricName:\"accuracy\"})\n",
    "lr_PREC  = lr_eval2.evaluate(predictions, {lr_eval2.metricName:\"weightedPrecision\"})\n",
    "lr_REC  = lr_eval2.evaluate(predictions, {lr_eval2.metricName:\"weightedRecall\"})\n",
    "lr_F1  = lr_eval2.evaluate(predictions, {lr_eval2.metricName:\"f1\"})\n",
    "print(\"Logistic Regression Performance Measure\")\n",
    "print(\"Accuracy = %0.8f\" % lr_ACC)\n",
    "print(\"Weighted Precision = %0.8f\" % lr_PREC)\n",
    "print(\"Weighted Recall = %0.8f\" % lr_REC)\n",
    "print(\"F1 = %0.8f\" % lr_F1)\n",
    "\n",
    "print(\"AUC = %.8f\" % lr_AUC)\n",
    "print('\\nEvaluation:')\n",
    "evaluate(predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 500x500 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "plt.figure(figsize=(5,5))\n",
    "plt.plot([0, 1], [0, 1], 'r--')\n",
    "plt.plot(lr_model.summary.roc.select('FPR').collect(),\n",
    "         lr_model.summary.roc.select('TPR').collect())\n",
    "plt.xlabel('FPR')\n",
    "plt.ylabel('TPR')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------------------------------+\n",
      "|probability                                |\n",
      "+-------------------------------------------+\n",
      "|[0.9999999999997708,2.291500322826323E-13] |\n",
      "|[0.9999999999986438,1.3562484468820912E-12]|\n",
      "|[0.9999999999986438,1.3562484468820912E-12]|\n",
      "+-------------------------------------------+\n",
      "only showing top 3 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "predictions.select(['probability']).show(3,truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "scoreAndLabels=predictions.select(['probability','label']).rdd.map(lambda row: (float(row['probability'][1]), float(row['label'])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "ename": "Py4JJavaError",
     "evalue": "An error occurred while calling z:org.apache.spark.api.python.PythonRDD.collectAndServe.\n: org.apache.spark.SparkException: Job aborted due to stage failure: Task 3 in stage 512.0 failed 1 times, most recent failure: Lost task 3.0 in stage 512.0 (TID 5019) (DESKTOP-JNOAOEJ executor driver): org.apache.spark.SparkException: Python worker failed to connect back.\r\n\tat org.apache.spark.api.python.PythonWorkerFactory.createSimpleWorker(PythonWorkerFactory.scala:189)\r\n\tat org.apache.spark.api.python.PythonWorkerFactory.create(PythonWorkerFactory.scala:109)\r\n\tat org.apache.spark.SparkEnv.createPythonWorker(SparkEnv.scala:124)\r\n\tat org.apache.spark.api.python.BasePythonRunner.compute(PythonRunner.scala:164)\r\n\tat org.apache.spark.api.python.PythonRDD.compute(PythonRDD.scala:65)\r\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:365)\r\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:329)\r\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)\r\n\tat org.apache.spark.scheduler.Task.run(Task.scala:136)\r\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:548)\r\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1504)\r\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:551)\r\n\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)\r\n\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)\r\n\tat java.base/java.lang.Thread.run(Thread.java:833)\r\nCaused by: java.net.SocketTimeoutException: Accept timed out\r\n\tat java.base/sun.nio.ch.NioSocketImpl.timedAccept(NioSocketImpl.java:708)\r\n\tat java.base/sun.nio.ch.NioSocketImpl.accept(NioSocketImpl.java:752)\r\n\tat java.base/java.net.ServerSocket.implAccept(ServerSocket.java:675)\r\n\tat java.base/java.net.ServerSocket.platformImplAccept(ServerSocket.java:641)\r\n\tat java.base/java.net.ServerSocket.implAccept(ServerSocket.java:617)\r\n\tat java.base/java.net.ServerSocket.implAccept(ServerSocket.java:574)\r\n\tat java.base/java.net.ServerSocket.accept(ServerSocket.java:532)\r\n\tat org.apache.spark.api.python.PythonWorkerFactory.createSimpleWorker(PythonWorkerFactory.scala:176)\r\n\t... 14 more\r\n\nDriver stacktrace:\r\n\tat org.apache.spark.scheduler.DAGScheduler.failJobAndIndependentStages(DAGScheduler.scala:2672)\r\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2(DAGScheduler.scala:2608)\r\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2$adapted(DAGScheduler.scala:2607)\r\n\tat scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62)\r\n\tat scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55)\r\n\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49)\r\n\tat org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:2607)\r\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1(DAGScheduler.scala:1182)\r\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1$adapted(DAGScheduler.scala:1182)\r\n\tat scala.Option.foreach(Option.scala:407)\r\n\tat org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:1182)\r\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:2860)\r\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2802)\r\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2791)\r\n\tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49)\r\n\tat org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:952)\r\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2228)\r\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2249)\r\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2268)\r\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2293)\r\n\tat org.apache.spark.rdd.RDD.$anonfun$collect$1(RDD.scala:1021)\r\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\r\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)\r\n\tat org.apache.spark.rdd.RDD.withScope(RDD.scala:406)\r\n\tat org.apache.spark.rdd.RDD.collect(RDD.scala:1020)\r\n\tat org.apache.spark.api.python.PythonRDD$.collectAndServe(PythonRDD.scala:180)\r\n\tat org.apache.spark.api.python.PythonRDD.collectAndServe(PythonRDD.scala)\r\n\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\r\n\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:77)\r\n\tat java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\r\n\tat java.base/java.lang.reflect.Method.invoke(Method.java:568)\r\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\r\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\r\n\tat py4j.Gateway.invoke(Gateway.java:282)\r\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\r\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\r\n\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)\r\n\tat py4j.ClientServerConnection.run(ClientServerConnection.java:106)\r\n\tat java.base/java.lang.Thread.run(Thread.java:833)\r\nCaused by: org.apache.spark.SparkException: Python worker failed to connect back.\r\n\tat org.apache.spark.api.python.PythonWorkerFactory.createSimpleWorker(PythonWorkerFactory.scala:189)\r\n\tat org.apache.spark.api.python.PythonWorkerFactory.create(PythonWorkerFactory.scala:109)\r\n\tat org.apache.spark.SparkEnv.createPythonWorker(SparkEnv.scala:124)\r\n\tat org.apache.spark.api.python.BasePythonRunner.compute(PythonRunner.scala:164)\r\n\tat org.apache.spark.api.python.PythonRDD.compute(PythonRDD.scala:65)\r\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:365)\r\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:329)\r\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)\r\n\tat org.apache.spark.scheduler.Task.run(Task.scala:136)\r\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:548)\r\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1504)\r\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:551)\r\n\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)\r\n\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)\r\n\t... 1 more\r\nCaused by: java.net.SocketTimeoutException: Accept timed out\r\n\tat java.base/sun.nio.ch.NioSocketImpl.timedAccept(NioSocketImpl.java:708)\r\n\tat java.base/sun.nio.ch.NioSocketImpl.accept(NioSocketImpl.java:752)\r\n\tat java.base/java.net.ServerSocket.implAccept(ServerSocket.java:675)\r\n\tat java.base/java.net.ServerSocket.platformImplAccept(ServerSocket.java:641)\r\n\tat java.base/java.net.ServerSocket.implAccept(ServerSocket.java:617)\r\n\tat java.base/java.net.ServerSocket.implAccept(ServerSocket.java:574)\r\n\tat java.base/java.net.ServerSocket.accept(ServerSocket.java:532)\r\n\tat org.apache.spark.api.python.PythonWorkerFactory.createSimpleWorker(PythonWorkerFactory.scala:176)\r\n\t... 14 more\r\n",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mPy4JJavaError\u001b[0m                             Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[86], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m scoreAndLabels\u001b[39m.\u001b[39;49mcollect()\n",
      "File \u001b[1;32mc:\\wamp64\\www\\spark_project\\venv\\lib\\site-packages\\pyspark\\rdd.py:1197\u001b[0m, in \u001b[0;36mRDD.collect\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   1195\u001b[0m \u001b[39mwith\u001b[39;00m SCCallSiteSync(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcontext):\n\u001b[0;32m   1196\u001b[0m     \u001b[39massert\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mctx\u001b[39m.\u001b[39m_jvm \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m\n\u001b[1;32m-> 1197\u001b[0m     sock_info \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mctx\u001b[39m.\u001b[39;49m_jvm\u001b[39m.\u001b[39;49mPythonRDD\u001b[39m.\u001b[39;49mcollectAndServe(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_jrdd\u001b[39m.\u001b[39;49mrdd())\n\u001b[0;32m   1198\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mlist\u001b[39m(_load_from_socket(sock_info, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_jrdd_deserializer))\n",
      "File \u001b[1;32mc:\\wamp64\\www\\spark_project\\venv\\lib\\site-packages\\py4j\\java_gateway.py:1321\u001b[0m, in \u001b[0;36mJavaMember.__call__\u001b[1;34m(self, *args)\u001b[0m\n\u001b[0;32m   1315\u001b[0m command \u001b[39m=\u001b[39m proto\u001b[39m.\u001b[39mCALL_COMMAND_NAME \u001b[39m+\u001b[39m\\\n\u001b[0;32m   1316\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcommand_header \u001b[39m+\u001b[39m\\\n\u001b[0;32m   1317\u001b[0m     args_command \u001b[39m+\u001b[39m\\\n\u001b[0;32m   1318\u001b[0m     proto\u001b[39m.\u001b[39mEND_COMMAND_PART\n\u001b[0;32m   1320\u001b[0m answer \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mgateway_client\u001b[39m.\u001b[39msend_command(command)\n\u001b[1;32m-> 1321\u001b[0m return_value \u001b[39m=\u001b[39m get_return_value(\n\u001b[0;32m   1322\u001b[0m     answer, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mgateway_client, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mtarget_id, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mname)\n\u001b[0;32m   1324\u001b[0m \u001b[39mfor\u001b[39;00m temp_arg \u001b[39min\u001b[39;00m temp_args:\n\u001b[0;32m   1325\u001b[0m     temp_arg\u001b[39m.\u001b[39m_detach()\n",
      "File \u001b[1;32mc:\\wamp64\\www\\spark_project\\venv\\lib\\site-packages\\pyspark\\sql\\utils.py:190\u001b[0m, in \u001b[0;36mcapture_sql_exception.<locals>.deco\u001b[1;34m(*a, **kw)\u001b[0m\n\u001b[0;32m    188\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mdeco\u001b[39m(\u001b[39m*\u001b[39ma: Any, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkw: Any) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m Any:\n\u001b[0;32m    189\u001b[0m     \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m--> 190\u001b[0m         \u001b[39mreturn\u001b[39;00m f(\u001b[39m*\u001b[39;49ma, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkw)\n\u001b[0;32m    191\u001b[0m     \u001b[39mexcept\u001b[39;00m Py4JJavaError \u001b[39mas\u001b[39;00m e:\n\u001b[0;32m    192\u001b[0m         converted \u001b[39m=\u001b[39m convert_exception(e\u001b[39m.\u001b[39mjava_exception)\n",
      "File \u001b[1;32mc:\\wamp64\\www\\spark_project\\venv\\lib\\site-packages\\py4j\\protocol.py:326\u001b[0m, in \u001b[0;36mget_return_value\u001b[1;34m(answer, gateway_client, target_id, name)\u001b[0m\n\u001b[0;32m    324\u001b[0m value \u001b[39m=\u001b[39m OUTPUT_CONVERTER[\u001b[39mtype\u001b[39m](answer[\u001b[39m2\u001b[39m:], gateway_client)\n\u001b[0;32m    325\u001b[0m \u001b[39mif\u001b[39;00m answer[\u001b[39m1\u001b[39m] \u001b[39m==\u001b[39m REFERENCE_TYPE:\n\u001b[1;32m--> 326\u001b[0m     \u001b[39mraise\u001b[39;00m Py4JJavaError(\n\u001b[0;32m    327\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mAn error occurred while calling \u001b[39m\u001b[39m{0}\u001b[39;00m\u001b[39m{1}\u001b[39;00m\u001b[39m{2}\u001b[39;00m\u001b[39m.\u001b[39m\u001b[39m\\n\u001b[39;00m\u001b[39m\"\u001b[39m\u001b[39m.\u001b[39m\n\u001b[0;32m    328\u001b[0m         \u001b[39mformat\u001b[39m(target_id, \u001b[39m\"\u001b[39m\u001b[39m.\u001b[39m\u001b[39m\"\u001b[39m, name), value)\n\u001b[0;32m    329\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m    330\u001b[0m     \u001b[39mraise\u001b[39;00m Py4JError(\n\u001b[0;32m    331\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mAn error occurred while calling \u001b[39m\u001b[39m{0}\u001b[39;00m\u001b[39m{1}\u001b[39;00m\u001b[39m{2}\u001b[39;00m\u001b[39m. Trace:\u001b[39m\u001b[39m\\n\u001b[39;00m\u001b[39m{3}\u001b[39;00m\u001b[39m\\n\u001b[39;00m\u001b[39m\"\u001b[39m\u001b[39m.\u001b[39m\n\u001b[0;32m    332\u001b[0m         \u001b[39mformat\u001b[39m(target_id, \u001b[39m\"\u001b[39m\u001b[39m.\u001b[39m\u001b[39m\"\u001b[39m, name, value))\n",
      "\u001b[1;31mPy4JJavaError\u001b[0m: An error occurred while calling z:org.apache.spark.api.python.PythonRDD.collectAndServe.\n: org.apache.spark.SparkException: Job aborted due to stage failure: Task 3 in stage 512.0 failed 1 times, most recent failure: Lost task 3.0 in stage 512.0 (TID 5019) (DESKTOP-JNOAOEJ executor driver): org.apache.spark.SparkException: Python worker failed to connect back.\r\n\tat org.apache.spark.api.python.PythonWorkerFactory.createSimpleWorker(PythonWorkerFactory.scala:189)\r\n\tat org.apache.spark.api.python.PythonWorkerFactory.create(PythonWorkerFactory.scala:109)\r\n\tat org.apache.spark.SparkEnv.createPythonWorker(SparkEnv.scala:124)\r\n\tat org.apache.spark.api.python.BasePythonRunner.compute(PythonRunner.scala:164)\r\n\tat org.apache.spark.api.python.PythonRDD.compute(PythonRDD.scala:65)\r\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:365)\r\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:329)\r\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)\r\n\tat org.apache.spark.scheduler.Task.run(Task.scala:136)\r\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:548)\r\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1504)\r\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:551)\r\n\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)\r\n\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)\r\n\tat java.base/java.lang.Thread.run(Thread.java:833)\r\nCaused by: java.net.SocketTimeoutException: Accept timed out\r\n\tat java.base/sun.nio.ch.NioSocketImpl.timedAccept(NioSocketImpl.java:708)\r\n\tat java.base/sun.nio.ch.NioSocketImpl.accept(NioSocketImpl.java:752)\r\n\tat java.base/java.net.ServerSocket.implAccept(ServerSocket.java:675)\r\n\tat java.base/java.net.ServerSocket.platformImplAccept(ServerSocket.java:641)\r\n\tat java.base/java.net.ServerSocket.implAccept(ServerSocket.java:617)\r\n\tat java.base/java.net.ServerSocket.implAccept(ServerSocket.java:574)\r\n\tat java.base/java.net.ServerSocket.accept(ServerSocket.java:532)\r\n\tat org.apache.spark.api.python.PythonWorkerFactory.createSimpleWorker(PythonWorkerFactory.scala:176)\r\n\t... 14 more\r\n\nDriver stacktrace:\r\n\tat org.apache.spark.scheduler.DAGScheduler.failJobAndIndependentStages(DAGScheduler.scala:2672)\r\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2(DAGScheduler.scala:2608)\r\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2$adapted(DAGScheduler.scala:2607)\r\n\tat scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62)\r\n\tat scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55)\r\n\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49)\r\n\tat org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:2607)\r\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1(DAGScheduler.scala:1182)\r\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1$adapted(DAGScheduler.scala:1182)\r\n\tat scala.Option.foreach(Option.scala:407)\r\n\tat org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:1182)\r\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:2860)\r\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2802)\r\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2791)\r\n\tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49)\r\n\tat org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:952)\r\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2228)\r\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2249)\r\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2268)\r\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2293)\r\n\tat org.apache.spark.rdd.RDD.$anonfun$collect$1(RDD.scala:1021)\r\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\r\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)\r\n\tat org.apache.spark.rdd.RDD.withScope(RDD.scala:406)\r\n\tat org.apache.spark.rdd.RDD.collect(RDD.scala:1020)\r\n\tat org.apache.spark.api.python.PythonRDD$.collectAndServe(PythonRDD.scala:180)\r\n\tat org.apache.spark.api.python.PythonRDD.collectAndServe(PythonRDD.scala)\r\n\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\r\n\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:77)\r\n\tat java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\r\n\tat java.base/java.lang.reflect.Method.invoke(Method.java:568)\r\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\r\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\r\n\tat py4j.Gateway.invoke(Gateway.java:282)\r\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\r\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\r\n\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)\r\n\tat py4j.ClientServerConnection.run(ClientServerConnection.java:106)\r\n\tat java.base/java.lang.Thread.run(Thread.java:833)\r\nCaused by: org.apache.spark.SparkException: Python worker failed to connect back.\r\n\tat org.apache.spark.api.python.PythonWorkerFactory.createSimpleWorker(PythonWorkerFactory.scala:189)\r\n\tat org.apache.spark.api.python.PythonWorkerFactory.create(PythonWorkerFactory.scala:109)\r\n\tat org.apache.spark.SparkEnv.createPythonWorker(SparkEnv.scala:124)\r\n\tat org.apache.spark.api.python.BasePythonRunner.compute(PythonRunner.scala:164)\r\n\tat org.apache.spark.api.python.PythonRDD.compute(PythonRDD.scala:65)\r\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:365)\r\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:329)\r\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)\r\n\tat org.apache.spark.scheduler.Task.run(Task.scala:136)\r\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:548)\r\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1504)\r\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:551)\r\n\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)\r\n\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)\r\n\t... 1 more\r\nCaused by: java.net.SocketTimeoutException: Accept timed out\r\n\tat java.base/sun.nio.ch.NioSocketImpl.timedAccept(NioSocketImpl.java:708)\r\n\tat java.base/sun.nio.ch.NioSocketImpl.accept(NioSocketImpl.java:752)\r\n\tat java.base/java.net.ServerSocket.implAccept(ServerSocket.java:675)\r\n\tat java.base/java.net.ServerSocket.platformImplAccept(ServerSocket.java:641)\r\n\tat java.base/java.net.ServerSocket.implAccept(ServerSocket.java:617)\r\n\tat java.base/java.net.ServerSocket.implAccept(ServerSocket.java:574)\r\n\tat java.base/java.net.ServerSocket.accept(ServerSocket.java:532)\r\n\tat org.apache.spark.api.python.PythonWorkerFactory.createSimpleWorker(PythonWorkerFactory.scala:176)\r\n\t... 14 more\r\n"
     ]
    }
   ],
   "source": [
    "scoreAndLabels.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\wamp64\\www\\spark_project\\venv\\lib\\site-packages\\pyspark\\sql\\context.py:157: FutureWarning: Deprecated in 3.0.0. Use SparkSession.builder.getOrCreate() instead.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "ename": "Py4JJavaError",
     "evalue": "An error occurred while calling z:org.apache.spark.api.python.PythonRDD.runJob.\n: org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 510.0 failed 1 times, most recent failure: Lost task 0.0 in stage 510.0 (TID 5014) (DESKTOP-JNOAOEJ executor driver): org.apache.spark.SparkException: Python worker failed to connect back.\r\n\tat org.apache.spark.api.python.PythonWorkerFactory.createSimpleWorker(PythonWorkerFactory.scala:189)\r\n\tat org.apache.spark.api.python.PythonWorkerFactory.create(PythonWorkerFactory.scala:109)\r\n\tat org.apache.spark.SparkEnv.createPythonWorker(SparkEnv.scala:124)\r\n\tat org.apache.spark.api.python.BasePythonRunner.compute(PythonRunner.scala:164)\r\n\tat org.apache.spark.api.python.PythonRDD.compute(PythonRDD.scala:65)\r\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:365)\r\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:329)\r\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)\r\n\tat org.apache.spark.scheduler.Task.run(Task.scala:136)\r\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:548)\r\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1504)\r\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:551)\r\n\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)\r\n\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)\r\n\tat java.base/java.lang.Thread.run(Thread.java:833)\r\nCaused by: java.net.SocketTimeoutException: Accept timed out\r\n\tat java.base/sun.nio.ch.NioSocketImpl.timedAccept(NioSocketImpl.java:708)\r\n\tat java.base/sun.nio.ch.NioSocketImpl.accept(NioSocketImpl.java:752)\r\n\tat java.base/java.net.ServerSocket.implAccept(ServerSocket.java:675)\r\n\tat java.base/java.net.ServerSocket.platformImplAccept(ServerSocket.java:641)\r\n\tat java.base/java.net.ServerSocket.implAccept(ServerSocket.java:617)\r\n\tat java.base/java.net.ServerSocket.implAccept(ServerSocket.java:574)\r\n\tat java.base/java.net.ServerSocket.accept(ServerSocket.java:532)\r\n\tat org.apache.spark.api.python.PythonWorkerFactory.createSimpleWorker(PythonWorkerFactory.scala:176)\r\n\t... 14 more\r\n\nDriver stacktrace:\r\n\tat org.apache.spark.scheduler.DAGScheduler.failJobAndIndependentStages(DAGScheduler.scala:2672)\r\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2(DAGScheduler.scala:2608)\r\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2$adapted(DAGScheduler.scala:2607)\r\n\tat scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62)\r\n\tat scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55)\r\n\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49)\r\n\tat org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:2607)\r\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1(DAGScheduler.scala:1182)\r\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1$adapted(DAGScheduler.scala:1182)\r\n\tat scala.Option.foreach(Option.scala:407)\r\n\tat org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:1182)\r\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:2860)\r\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2802)\r\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2791)\r\n\tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49)\r\n\tat org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:952)\r\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2228)\r\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2249)\r\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2268)\r\n\tat org.apache.spark.api.python.PythonRDD$.runJob(PythonRDD.scala:166)\r\n\tat org.apache.spark.api.python.PythonRDD.runJob(PythonRDD.scala)\r\n\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\r\n\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:77)\r\n\tat java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\r\n\tat java.base/java.lang.reflect.Method.invoke(Method.java:568)\r\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\r\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\r\n\tat py4j.Gateway.invoke(Gateway.java:282)\r\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\r\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\r\n\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)\r\n\tat py4j.ClientServerConnection.run(ClientServerConnection.java:106)\r\n\tat java.base/java.lang.Thread.run(Thread.java:833)\r\nCaused by: org.apache.spark.SparkException: Python worker failed to connect back.\r\n\tat org.apache.spark.api.python.PythonWorkerFactory.createSimpleWorker(PythonWorkerFactory.scala:189)\r\n\tat org.apache.spark.api.python.PythonWorkerFactory.create(PythonWorkerFactory.scala:109)\r\n\tat org.apache.spark.SparkEnv.createPythonWorker(SparkEnv.scala:124)\r\n\tat org.apache.spark.api.python.BasePythonRunner.compute(PythonRunner.scala:164)\r\n\tat org.apache.spark.api.python.PythonRDD.compute(PythonRDD.scala:65)\r\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:365)\r\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:329)\r\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)\r\n\tat org.apache.spark.scheduler.Task.run(Task.scala:136)\r\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:548)\r\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1504)\r\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:551)\r\n\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)\r\n\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)\r\n\t... 1 more\r\nCaused by: java.net.SocketTimeoutException: Accept timed out\r\n\tat java.base/sun.nio.ch.NioSocketImpl.timedAccept(NioSocketImpl.java:708)\r\n\tat java.base/sun.nio.ch.NioSocketImpl.accept(NioSocketImpl.java:752)\r\n\tat java.base/java.net.ServerSocket.implAccept(ServerSocket.java:675)\r\n\tat java.base/java.net.ServerSocket.platformImplAccept(ServerSocket.java:641)\r\n\tat java.base/java.net.ServerSocket.implAccept(ServerSocket.java:617)\r\n\tat java.base/java.net.ServerSocket.implAccept(ServerSocket.java:574)\r\n\tat java.base/java.net.ServerSocket.accept(ServerSocket.java:532)\r\n\tat org.apache.spark.api.python.PythonWorkerFactory.createSimpleWorker(PythonWorkerFactory.scala:176)\r\n\t... 14 more\r\n",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mPy4JJavaError\u001b[0m                             Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[83], line 3\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mmatplotlib\u001b[39;00m \u001b[39mimport\u001b[39;00m pyplot \u001b[39mas\u001b[39;00m plt\n\u001b[0;32m      2\u001b[0m get_ipython()\u001b[39m.\u001b[39mrun_line_magic(\u001b[39m'\u001b[39m\u001b[39mmatplotlib\u001b[39m\u001b[39m'\u001b[39m, \u001b[39m'\u001b[39m\u001b[39minline\u001b[39m\u001b[39m'\u001b[39m)\n\u001b[1;32m----> 3\u001b[0m bcm \u001b[39m=\u001b[39m BinaryClassificationMetrics(scoreAndLabels)\n\u001b[0;32m      4\u001b[0m \u001b[39m# We still can get the same metrics as the evaluator...\u001b[39;00m\n\u001b[0;32m      5\u001b[0m \u001b[39mprint\u001b[39m(\u001b[39m\"\u001b[39m\u001b[39mArea under ROC Curve: \u001b[39m\u001b[39m{:.4f}\u001b[39;00m\u001b[39m\"\u001b[39m\u001b[39m.\u001b[39mformat(bcm\u001b[39m.\u001b[39mareaUnderROC))\n",
      "File \u001b[1;32mc:\\wamp64\\www\\spark_project\\venv\\lib\\site-packages\\pyspark\\mllib\\evaluation.py:73\u001b[0m, in \u001b[0;36mBinaryClassificationMetrics.__init__\u001b[1;34m(self, scoreAndLabels)\u001b[0m\n\u001b[0;32m     71\u001b[0m sc \u001b[39m=\u001b[39m scoreAndLabels\u001b[39m.\u001b[39mctx\n\u001b[0;32m     72\u001b[0m sql_ctx \u001b[39m=\u001b[39m SQLContext\u001b[39m.\u001b[39mgetOrCreate(sc)\n\u001b[1;32m---> 73\u001b[0m numCol \u001b[39m=\u001b[39m \u001b[39mlen\u001b[39m(scoreAndLabels\u001b[39m.\u001b[39;49mfirst())\n\u001b[0;32m     74\u001b[0m schema \u001b[39m=\u001b[39m StructType(\n\u001b[0;32m     75\u001b[0m     [\n\u001b[0;32m     76\u001b[0m         StructField(\u001b[39m\"\u001b[39m\u001b[39mscore\u001b[39m\u001b[39m\"\u001b[39m, DoubleType(), nullable\u001b[39m=\u001b[39m\u001b[39mFalse\u001b[39;00m),\n\u001b[0;32m     77\u001b[0m         StructField(\u001b[39m\"\u001b[39m\u001b[39mlabel\u001b[39m\u001b[39m\"\u001b[39m, DoubleType(), nullable\u001b[39m=\u001b[39m\u001b[39mFalse\u001b[39;00m),\n\u001b[0;32m     78\u001b[0m     ]\n\u001b[0;32m     79\u001b[0m )\n\u001b[0;32m     80\u001b[0m \u001b[39mif\u001b[39;00m numCol \u001b[39m==\u001b[39m \u001b[39m3\u001b[39m:\n",
      "File \u001b[1;32mc:\\wamp64\\www\\spark_project\\venv\\lib\\site-packages\\pyspark\\rdd.py:1903\u001b[0m, in \u001b[0;36mRDD.first\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   1890\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mfirst\u001b[39m(\u001b[39mself\u001b[39m: \u001b[39m\"\u001b[39m\u001b[39mRDD[T]\u001b[39m\u001b[39m\"\u001b[39m) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m T:\n\u001b[0;32m   1891\u001b[0m \u001b[39m    \u001b[39m\u001b[39m\"\"\"\u001b[39;00m\n\u001b[0;32m   1892\u001b[0m \u001b[39m    Return the first element in this RDD.\u001b[39;00m\n\u001b[0;32m   1893\u001b[0m \n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1901\u001b[0m \u001b[39m    ValueError: RDD is empty\u001b[39;00m\n\u001b[0;32m   1902\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[1;32m-> 1903\u001b[0m     rs \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mtake(\u001b[39m1\u001b[39;49m)\n\u001b[0;32m   1904\u001b[0m     \u001b[39mif\u001b[39;00m rs:\n\u001b[0;32m   1905\u001b[0m         \u001b[39mreturn\u001b[39;00m rs[\u001b[39m0\u001b[39m]\n",
      "File \u001b[1;32mc:\\wamp64\\www\\spark_project\\venv\\lib\\site-packages\\pyspark\\rdd.py:1883\u001b[0m, in \u001b[0;36mRDD.take\u001b[1;34m(self, num)\u001b[0m\n\u001b[0;32m   1880\u001b[0m         taken \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m \u001b[39m1\u001b[39m\n\u001b[0;32m   1882\u001b[0m p \u001b[39m=\u001b[39m \u001b[39mrange\u001b[39m(partsScanned, \u001b[39mmin\u001b[39m(partsScanned \u001b[39m+\u001b[39m numPartsToTry, totalParts))\n\u001b[1;32m-> 1883\u001b[0m res \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mcontext\u001b[39m.\u001b[39;49mrunJob(\u001b[39mself\u001b[39;49m, takeUpToNumLeft, p)\n\u001b[0;32m   1885\u001b[0m items \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m res\n\u001b[0;32m   1886\u001b[0m partsScanned \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m numPartsToTry\n",
      "File \u001b[1;32mc:\\wamp64\\www\\spark_project\\venv\\lib\\site-packages\\pyspark\\context.py:1486\u001b[0m, in \u001b[0;36mSparkContext.runJob\u001b[1;34m(self, rdd, partitionFunc, partitions, allowLocal)\u001b[0m\n\u001b[0;32m   1484\u001b[0m mappedRDD \u001b[39m=\u001b[39m rdd\u001b[39m.\u001b[39mmapPartitions(partitionFunc)\n\u001b[0;32m   1485\u001b[0m \u001b[39massert\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_jvm \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m\n\u001b[1;32m-> 1486\u001b[0m sock_info \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_jvm\u001b[39m.\u001b[39;49mPythonRDD\u001b[39m.\u001b[39;49mrunJob(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_jsc\u001b[39m.\u001b[39;49msc(), mappedRDD\u001b[39m.\u001b[39;49m_jrdd, partitions)\n\u001b[0;32m   1487\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mlist\u001b[39m(_load_from_socket(sock_info, mappedRDD\u001b[39m.\u001b[39m_jrdd_deserializer))\n",
      "File \u001b[1;32mc:\\wamp64\\www\\spark_project\\venv\\lib\\site-packages\\py4j\\java_gateway.py:1321\u001b[0m, in \u001b[0;36mJavaMember.__call__\u001b[1;34m(self, *args)\u001b[0m\n\u001b[0;32m   1315\u001b[0m command \u001b[39m=\u001b[39m proto\u001b[39m.\u001b[39mCALL_COMMAND_NAME \u001b[39m+\u001b[39m\\\n\u001b[0;32m   1316\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcommand_header \u001b[39m+\u001b[39m\\\n\u001b[0;32m   1317\u001b[0m     args_command \u001b[39m+\u001b[39m\\\n\u001b[0;32m   1318\u001b[0m     proto\u001b[39m.\u001b[39mEND_COMMAND_PART\n\u001b[0;32m   1320\u001b[0m answer \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mgateway_client\u001b[39m.\u001b[39msend_command(command)\n\u001b[1;32m-> 1321\u001b[0m return_value \u001b[39m=\u001b[39m get_return_value(\n\u001b[0;32m   1322\u001b[0m     answer, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mgateway_client, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mtarget_id, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mname)\n\u001b[0;32m   1324\u001b[0m \u001b[39mfor\u001b[39;00m temp_arg \u001b[39min\u001b[39;00m temp_args:\n\u001b[0;32m   1325\u001b[0m     temp_arg\u001b[39m.\u001b[39m_detach()\n",
      "File \u001b[1;32mc:\\wamp64\\www\\spark_project\\venv\\lib\\site-packages\\pyspark\\sql\\utils.py:190\u001b[0m, in \u001b[0;36mcapture_sql_exception.<locals>.deco\u001b[1;34m(*a, **kw)\u001b[0m\n\u001b[0;32m    188\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mdeco\u001b[39m(\u001b[39m*\u001b[39ma: Any, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkw: Any) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m Any:\n\u001b[0;32m    189\u001b[0m     \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m--> 190\u001b[0m         \u001b[39mreturn\u001b[39;00m f(\u001b[39m*\u001b[39;49ma, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkw)\n\u001b[0;32m    191\u001b[0m     \u001b[39mexcept\u001b[39;00m Py4JJavaError \u001b[39mas\u001b[39;00m e:\n\u001b[0;32m    192\u001b[0m         converted \u001b[39m=\u001b[39m convert_exception(e\u001b[39m.\u001b[39mjava_exception)\n",
      "File \u001b[1;32mc:\\wamp64\\www\\spark_project\\venv\\lib\\site-packages\\py4j\\protocol.py:326\u001b[0m, in \u001b[0;36mget_return_value\u001b[1;34m(answer, gateway_client, target_id, name)\u001b[0m\n\u001b[0;32m    324\u001b[0m value \u001b[39m=\u001b[39m OUTPUT_CONVERTER[\u001b[39mtype\u001b[39m](answer[\u001b[39m2\u001b[39m:], gateway_client)\n\u001b[0;32m    325\u001b[0m \u001b[39mif\u001b[39;00m answer[\u001b[39m1\u001b[39m] \u001b[39m==\u001b[39m REFERENCE_TYPE:\n\u001b[1;32m--> 326\u001b[0m     \u001b[39mraise\u001b[39;00m Py4JJavaError(\n\u001b[0;32m    327\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mAn error occurred while calling \u001b[39m\u001b[39m{0}\u001b[39;00m\u001b[39m{1}\u001b[39;00m\u001b[39m{2}\u001b[39;00m\u001b[39m.\u001b[39m\u001b[39m\\n\u001b[39;00m\u001b[39m\"\u001b[39m\u001b[39m.\u001b[39m\n\u001b[0;32m    328\u001b[0m         \u001b[39mformat\u001b[39m(target_id, \u001b[39m\"\u001b[39m\u001b[39m.\u001b[39m\u001b[39m\"\u001b[39m, name), value)\n\u001b[0;32m    329\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m    330\u001b[0m     \u001b[39mraise\u001b[39;00m Py4JError(\n\u001b[0;32m    331\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mAn error occurred while calling \u001b[39m\u001b[39m{0}\u001b[39;00m\u001b[39m{1}\u001b[39;00m\u001b[39m{2}\u001b[39;00m\u001b[39m. Trace:\u001b[39m\u001b[39m\\n\u001b[39;00m\u001b[39m{3}\u001b[39;00m\u001b[39m\\n\u001b[39;00m\u001b[39m\"\u001b[39m\u001b[39m.\u001b[39m\n\u001b[0;32m    332\u001b[0m         \u001b[39mformat\u001b[39m(target_id, \u001b[39m\"\u001b[39m\u001b[39m.\u001b[39m\u001b[39m\"\u001b[39m, name, value))\n",
      "\u001b[1;31mPy4JJavaError\u001b[0m: An error occurred while calling z:org.apache.spark.api.python.PythonRDD.runJob.\n: org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 510.0 failed 1 times, most recent failure: Lost task 0.0 in stage 510.0 (TID 5014) (DESKTOP-JNOAOEJ executor driver): org.apache.spark.SparkException: Python worker failed to connect back.\r\n\tat org.apache.spark.api.python.PythonWorkerFactory.createSimpleWorker(PythonWorkerFactory.scala:189)\r\n\tat org.apache.spark.api.python.PythonWorkerFactory.create(PythonWorkerFactory.scala:109)\r\n\tat org.apache.spark.SparkEnv.createPythonWorker(SparkEnv.scala:124)\r\n\tat org.apache.spark.api.python.BasePythonRunner.compute(PythonRunner.scala:164)\r\n\tat org.apache.spark.api.python.PythonRDD.compute(PythonRDD.scala:65)\r\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:365)\r\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:329)\r\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)\r\n\tat org.apache.spark.scheduler.Task.run(Task.scala:136)\r\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:548)\r\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1504)\r\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:551)\r\n\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)\r\n\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)\r\n\tat java.base/java.lang.Thread.run(Thread.java:833)\r\nCaused by: java.net.SocketTimeoutException: Accept timed out\r\n\tat java.base/sun.nio.ch.NioSocketImpl.timedAccept(NioSocketImpl.java:708)\r\n\tat java.base/sun.nio.ch.NioSocketImpl.accept(NioSocketImpl.java:752)\r\n\tat java.base/java.net.ServerSocket.implAccept(ServerSocket.java:675)\r\n\tat java.base/java.net.ServerSocket.platformImplAccept(ServerSocket.java:641)\r\n\tat java.base/java.net.ServerSocket.implAccept(ServerSocket.java:617)\r\n\tat java.base/java.net.ServerSocket.implAccept(ServerSocket.java:574)\r\n\tat java.base/java.net.ServerSocket.accept(ServerSocket.java:532)\r\n\tat org.apache.spark.api.python.PythonWorkerFactory.createSimpleWorker(PythonWorkerFactory.scala:176)\r\n\t... 14 more\r\n\nDriver stacktrace:\r\n\tat org.apache.spark.scheduler.DAGScheduler.failJobAndIndependentStages(DAGScheduler.scala:2672)\r\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2(DAGScheduler.scala:2608)\r\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2$adapted(DAGScheduler.scala:2607)\r\n\tat scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62)\r\n\tat scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55)\r\n\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49)\r\n\tat org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:2607)\r\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1(DAGScheduler.scala:1182)\r\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1$adapted(DAGScheduler.scala:1182)\r\n\tat scala.Option.foreach(Option.scala:407)\r\n\tat org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:1182)\r\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:2860)\r\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2802)\r\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2791)\r\n\tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49)\r\n\tat org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:952)\r\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2228)\r\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2249)\r\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2268)\r\n\tat org.apache.spark.api.python.PythonRDD$.runJob(PythonRDD.scala:166)\r\n\tat org.apache.spark.api.python.PythonRDD.runJob(PythonRDD.scala)\r\n\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\r\n\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:77)\r\n\tat java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\r\n\tat java.base/java.lang.reflect.Method.invoke(Method.java:568)\r\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\r\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\r\n\tat py4j.Gateway.invoke(Gateway.java:282)\r\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\r\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\r\n\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)\r\n\tat py4j.ClientServerConnection.run(ClientServerConnection.java:106)\r\n\tat java.base/java.lang.Thread.run(Thread.java:833)\r\nCaused by: org.apache.spark.SparkException: Python worker failed to connect back.\r\n\tat org.apache.spark.api.python.PythonWorkerFactory.createSimpleWorker(PythonWorkerFactory.scala:189)\r\n\tat org.apache.spark.api.python.PythonWorkerFactory.create(PythonWorkerFactory.scala:109)\r\n\tat org.apache.spark.SparkEnv.createPythonWorker(SparkEnv.scala:124)\r\n\tat org.apache.spark.api.python.BasePythonRunner.compute(PythonRunner.scala:164)\r\n\tat org.apache.spark.api.python.PythonRDD.compute(PythonRDD.scala:65)\r\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:365)\r\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:329)\r\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)\r\n\tat org.apache.spark.scheduler.Task.run(Task.scala:136)\r\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:548)\r\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1504)\r\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:551)\r\n\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)\r\n\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)\r\n\t... 1 more\r\nCaused by: java.net.SocketTimeoutException: Accept timed out\r\n\tat java.base/sun.nio.ch.NioSocketImpl.timedAccept(NioSocketImpl.java:708)\r\n\tat java.base/sun.nio.ch.NioSocketImpl.accept(NioSocketImpl.java:752)\r\n\tat java.base/java.net.ServerSocket.implAccept(ServerSocket.java:675)\r\n\tat java.base/java.net.ServerSocket.platformImplAccept(ServerSocket.java:641)\r\n\tat java.base/java.net.ServerSocket.implAccept(ServerSocket.java:617)\r\n\tat java.base/java.net.ServerSocket.implAccept(ServerSocket.java:574)\r\n\tat java.base/java.net.ServerSocket.accept(ServerSocket.java:532)\r\n\tat org.apache.spark.api.python.PythonWorkerFactory.createSimpleWorker(PythonWorkerFactory.scala:176)\r\n\t... 14 more\r\n"
     ]
    }
   ],
   "source": [
    "from matplotlib import pyplot as plt\n",
    "%matplotlib inline\n",
    "bcm = BinaryClassificationMetrics(scoreAndLabels)\n",
    "# We still can get the same metrics as the evaluator...\n",
    "print(\"Area under ROC Curve: {:.4f}\".format(bcm.areaUnderROC))\n",
    "print(\"Area under PR Curve: {:.4f}\".format(bcm.areaUnderPR))\n",
    "\n",
    "# But now we can PLOT both ROC and PR curves!\n",
    "fig, axs = plt.subplots(1, 2, figsize=(12, 4))\n",
    "bcm.plot_roc_curve(ax=axs[0])\n",
    "bcm.plot_pr_curve(ax=axs[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+----------+-----------------------------------------+\n",
      "|label|prediction|probability                              |\n",
      "+-----+----------+-----------------------------------------+\n",
      "|0    |0.0       |[0.9999965769719209,3.423028079099333E-6]|\n",
      "|0    |0.0       |[0.9999965769719209,3.423028079099333E-6]|\n",
      "|0    |0.0       |[0.9999965769719209,3.423028079099333E-6]|\n",
      "|0    |0.0       |[0.9999965769719209,3.423028079099333E-6]|\n",
      "|0    |0.0       |[0.9999965769719209,3.423028079099333E-6]|\n",
      "+-----+----------+-----------------------------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "# Create a classifier object and fit to the training data\n",
    "tree = DecisionTreeClassifier()\n",
    "tree_model = tree.fit(train)\n",
    "\n",
    "# Create predictions for the testing data and take a look at the predictions\n",
    "prediction = tree_model.transform(test)\n",
    "prediction.select('label', 'prediction', 'probability').show(5, False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def evaluate(pred):\n",
    "    pred.groupBy('label', 'prediction').count().show()\n",
    "\n",
    "    # Calculate the elements of the confusion matrix\n",
    "    TN = pred.filter('prediction = 0 AND label = prediction').count()\n",
    "    TP = pred.filter('prediction = 1 AND label = prediction').count()\n",
    "    FN = pred.filter('prediction = 0 AND label = 1').count()\n",
    "    FP = pred.filter('prediction = 1 AND label = 0').count()\n",
    "\n",
    "    # Accuracy measures the proportion of correct predictions\n",
    "    accuracy = (TN + TP) / (TN + TP + FN + FP)\n",
    "    recall = (TP) / (TP+FN)\n",
    "    precision= (TP) / (TP+FP)\n",
    "    f1=2*(precision*recall)/(precision+recall)\n",
    "    print('EVALUATION SUMMARY:')\n",
    "    print(f'accuracy:{accuracy} \\nprecision:{precision} \\nrecall:{recall} \\nf1-score:{f1}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+----------+-------+\n",
      "|label|prediction|  count|\n",
      "+-----+----------+-------+\n",
      "|    1|       0.0|     33|\n",
      "|    0|       0.0|1717323|\n",
      "|    1|       1.0|   6166|\n",
      "|    0|       1.0|     93|\n",
      "+-----+----------+-------+\n",
      "\n",
      "EVALUATION SUMMARY:\n",
      "accuracy:0.9999268978281113 \n",
      "precision:0.9851413963891995 \n",
      "recall:0.9946765607356025 \n",
      "f1-score:0.9898860170171776\n"
     ]
    }
   ],
   "source": [
    "evaluate(prediction)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+----------+-------------------------------------------+\n",
      "|label|prediction|probability                                |\n",
      "+-----+----------+-------------------------------------------+\n",
      "|0    |0.0       |[0.9999999999783138,2.1686208384608108E-11]|\n",
      "|0    |0.0       |[0.9999999999262756,7.372435995023352E-11] |\n",
      "|0    |0.0       |[0.9999999997835478,2.1645218950538947E-10]|\n",
      "|0    |0.0       |[1.0,0.0]                                  |\n",
      "|0    |0.0       |[0.9999999999999998,2.220446049250313E-16] |\n",
      "+-----+----------+-------------------------------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.ml.classification import LogisticRegression\n",
    "from pyspark.ml import Pipeline\n",
    "logistic = LogisticRegression()\n",
    "lr_pipeline = Pipeline(stages=[ logistic])\n",
    "lr_model=lr_pipeline.fit(train)\n",
    "lr_prediction=lr_model.transform(test)\n",
    "lr_prediction.select('label', 'prediction', 'probability').show(5, False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+----------+-------+\n",
      "|label|prediction|  count|\n",
      "+-----+----------+-------+\n",
      "|    0|       0.0|1717405|\n",
      "|    1|       1.0|   6181|\n",
      "|    0|       1.0|     11|\n",
      "|    1|       0.0|     18|\n",
      "+-----+----------+-------+\n",
      "\n",
      "EVALUATION SUMMARY:\n",
      "accuracy:0.9999831748969462 \n",
      "precision:0.9982235142118863 \n",
      "recall:0.9970963058557832 \n",
      "f1-score:0.9976595916390929\n"
     ]
    }
   ],
   "source": [
    "evaluate(lr_prediction)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+----------+------------------------------------------+\n",
      "|label|prediction|probability                               |\n",
      "+-----+----------+------------------------------------------+\n",
      "|0    |0.0       |[0.9999644889923797,3.5511007620311594E-5]|\n",
      "|0    |0.0       |[0.9999720909893208,2.790901067927771E-5] |\n",
      "|0    |0.0       |[0.9999644889923797,3.5511007620311594E-5]|\n",
      "|0    |0.0       |[0.9999838082776951,1.6191722304915428E-5]|\n",
      "|0    |0.0       |[0.9999838082776951,1.6191722304915428E-5]|\n",
      "+-----+----------+------------------------------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.ml.classification import RandomForestClassifier\n",
    "from pyspark.ml import Pipeline\n",
    "rf = RandomForestClassifier(labelCol=\"label\", featuresCol=\"features\")\n",
    "pipeline = Pipeline(stages=[ rf])\n",
    "rf_model=pipeline.fit(train)\n",
    "rf_predictions=rf_model.transform(test)\n",
    "rf_predictions.select('label', 'prediction', 'probability').show(5, False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['is_match',\n",
       " 'cmp_fname_c1_imputed',\n",
       " 'cmp_fname_c2_imputed',\n",
       " 'cmp_lname_c1_imputed',\n",
       " 'cmp_lname_c2_imputed',\n",
       " 'cmp_sex_imputed',\n",
       " 'cmp_bd_imputed',\n",
       " 'cmp_bm_imputed',\n",
       " 'cmp_by_imputed',\n",
       " 'cmp_plz_imputed',\n",
       " 'label',\n",
       " 'features',\n",
       " 'rawPrediction',\n",
       " 'probability',\n",
       " 'prediction']"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rf_predictions.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+----------+-------+\n",
      "|label|prediction|  count|\n",
      "+-----+----------+-------+\n",
      "|    1|       0.0|     73|\n",
      "|    0|       0.0|1717414|\n",
      "|    1|       1.0|   6126|\n",
      "|    0|       1.0|      2|\n",
      "+-----+----------+-------+\n",
      "\n",
      "EVALUATION SUMMARY:\n",
      "accuracy:0.9999564868024472 \n",
      "precision:0.9996736292428199 \n",
      "recall:0.9882239070817874 \n",
      "f1-score:0.9939157945972257\n"
     ]
    }
   ],
   "source": [
    "evaluate(rf_predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# todo: implement area under ROC \n",
    "\n",
    "# predictionAndLabels = prediction.select('prediction','label').rdd \\\n",
    "#                             .map(lambda x: (x[0],x[1]))\n",
    "\n",
    "\n",
    "# # Instantiate metrics object\n",
    "# metrics = BinaryClassificationMetrics(predictionAndLabels)\n",
    "# # Area under ROC curve\n",
    "# print(\"Area under ROC = %s\" % metrics.areaUnderROC)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cross Validation "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.tuning import CrossValidator, ParamGridBuilder, CrossValidatorModel\n",
    "from pyspark.ml.evaluation import BinaryClassificationEvaluator\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr_cs = LogisticRegression()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.classification import LogisticRegression\n",
    "lr_cs = LogisticRegression()\n",
    "grid = ParamGridBuilder().addGrid(lr_cs.regParam, [0.1, 0.01]) \\\n",
    "    .addGrid(lr_cs.fitIntercept, [False, True])\\\n",
    "    .addGrid(lr_cs.elasticNetParam, [0.0, 1.0])\\\n",
    "    .build()\n",
    "evaluator = BinaryClassificationEvaluator()\n",
    "cv = CrossValidator(estimator=lr_cs, estimatorParamMaps=grid, evaluator=evaluator)\n",
    "cvModel = cv.fit(train)\n",
    "lrprediction=cvModel.transform(test)\n",
    "\n",
    "\n",
    "\n",
    "print('Accuracy:', evaluator.evaluate(lrprediction))\n",
    "print('AUC:', BinaryClassificationMetrics(lrprediction['label','prediction'].rdd).areaUnderROC)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluate(lrprediction)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.classification import DecisionTreeClassifier\n",
    "\n",
    "dt = DecisionTreeClassifier()\n",
    "grid = ParamGridBuilder() \\\n",
    "        .addGrid(dt.maxDepth,  [2, 5, 10, 20, 30]) \\\n",
    "        .addGrid(dt.maxBins,  [10, 20, 40, 80, 100]) \\\n",
    "        .build()\n",
    "dtevaluator = BinaryClassificationEvaluator(rawPredictionCol=\"rawPrediction\")\n",
    "cv = CrossValidator(estimator=dt, \n",
    "                    estimatorParamMaps=grid, \n",
    "                    evaluator=dtevaluator,\n",
    "                    numFolds = 3)\n",
    "dtModel = cv.fit(train)\n",
    "dtpredictions = dtModel.transform(test)\n",
    "\n",
    "print('Accuracy:', dtevaluator.evaluate(dtpredictions))\n",
    "print('AUC:', BinaryClassificationMetrics(dtpredictions['label','prediction'].rdd).areaUnderROC)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluate(dtpredictions)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "6ed0a4610ee5d358854c63a9c9df0be609d14f26b47cd9f541e7269cbaa1b618"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
