# معیارهای ارزیابی
در اینجا ابتدا به داده نامتوازن پرداخته و سپس معیارهای ارزیابی متناسب آن را معرفی می‌کنیم.
## داده نامتوازن
داده‌های نامتوازن یا imabalanced data به مجموعه داده‌ای گفته می‌شود که تعداد نمونه‌های هرکلاس آن با یکدیگر اختلاف فاحشی دارند.
به طوری که اگر یک مدل یادگیری ماشینی بر روی آن آموزش دهیم، منجر به یادگیری دقیق کلاس اکثریت و عدم یادگیری کلاس اقلیت می‌شود. 
برای مواجهه با داده نامتوازن چندین روش پیشنهاد شده است:
1. استفاده از سنجه‌هایی مثل f1 (macro)، AUC.
2. نمونه‌گیری متوازن و استفاده از متریک‌های معمولی مثل accuracy.
3. وزن دهی متناسب به loss که در هنگام ساخت مدل وزن‌دهی را تعریف می‌کنیم.

در این پروژه از روش اول یعنی استفاده از سنجه‌های متناسب با نامتوازنی داده جهت مقایسه مدل‌ها با یکدیگر استفاده می‌کنیم.

## سنجه F1(macro)
در حالت کلی سنجه F1 میانگین هارمونیک دو سنجه دقت(precision) و یادآوری (recall) است. برای رفع ابهام، نام‌های دیگر این سنجه همراه محاسبه آن آورده شده است:


1. precision or positive predictive value (PPV)

  $$TNR = \frac{TP}{TP+FP}$$

2. sensitivity, recall, hit rate, or true positive rate (TPR)

  $$TNR = \frac{TP}{P}=\frac{TP}{TP+FN}$$

به زبان ساده، دقت نشان می‌دهد که از بین پیش‌بینی‌های مدل برای کلاس True، چه نسبتی صحیح پیش‌بینی شده است.
یادآوری هم نشان می‌دهد از بین تمام داده‌های کلاس True، چه نسبتی درست پیش‌بینی شده است.
با توجه به اینکه مقایسه دو سنجه از یک مدل با دو سنجه از یک مدل دیگر کار دشواری است، یک سنجه F1 که میانگین هارمونیک این دو است معرفی شده است که به صورت زیر است:
$$F1 = 2 * \frac{Precision*Recall}{{Precision+Recall}}$$

تمام این سنجه‌ها در حالت عالی نمایانگر قدرت پیش‌بینی مدل است. اما وقتی داده نامتوازن باشد، باید از حالت خاصی از F1 یعنی F1 macro استفاده کرد.
سنجهF1 macro میانگین غیر وزن‌دار f1 برای تمام کلاس‌ها را اندازه می‌گیرد(N تعداد کلاس‌ها است).


$$F1(macro) =  \frac{\sum f1_{i}}{N}$$
## منحنی ROC


## منابع
- https://en.wikipedia.org/wiki/Sensitivity_and_specificity
- https://stephenallwright.com/micro-vs-macro-f1-score/
